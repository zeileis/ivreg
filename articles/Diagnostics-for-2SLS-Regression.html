<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Diagnostics for 2SLS Regression • ivreg</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Diagnostics for 2SLS Regression">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ivreg</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.6-4</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../articles/ivreg.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Articles

    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Diagnostics-for-2SLS-Regression.html">Diagnostics for 2SLS Regression</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/zeileis/ivreg/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Diagnostics for 2SLS Regression</h1>
                        <h4 data-toc-skip class="author">John Fox,
Christian Kleiber, Achim Zeileis</h4>
            
            <h4 data-toc-skip class="date">last modified:
2023-05-16</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/zeileis/ivreg/blob/main/vignettes/Diagnostics-for-2SLS-Regression.Rmd" class="external-link"><code>vignettes/Diagnostics-for-2SLS-Regression.Rmd</code></a></small>
      <div class="hidden name"><code>Diagnostics-for-2SLS-Regression.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>The <strong>ivreg</strong> package extends a variety of standard
numeric and graphical regression diagnostics to linear models fit by
<em>two-stage least-squares (2SLS)</em> regression, a commonly employed
method of instrumental-variables estimation for potentially
overidentified structural equations in which there are endogenous
regressors <span class="citation">(see e.g., Greene 2003)</span>. The
<code><a href="../reference/ivreg.fit.html">ivreg.fit()</a></code> function in the package computes the 2SLS
estimator employing a low-level interface not generally intended for
direct use, and returns a list containing quantities that facilitate the
computation of various diagnostics. The <code><a href="../reference/ivreg.html">ivreg()</a></code> function
provides a user-friendly formula-based interface to
<code><a href="../reference/ivreg.fit.html">ivreg.fit()</a></code>.</p>
<p><code><a href="../reference/ivreg.html">ivreg()</a></code> is derived from and supersedes the
<code><a href="../reference/ivreg.html">ivreg()</a></code> function in the <strong>AER</strong> package <span class="citation">(Kleiber and Zeileis 2008)</span>, making additional
provision for regression diagnostics. The principal subject of this
vignette is the rationale for the extension of various standard
regression diagnostics to 2SLS and the use of functions in the
<strong>ivreg</strong> package to compute them, along with functions in
other packages, specifically the base-R <strong>stats</strong> package
<span class="citation">(R Core Team 2020)</span> and the
<strong>car</strong> and <strong>effects</strong> packages <span class="citation">(Fox and Weisberg 2019)</span>, that work with the
<code>"ivreg"</code> objects produced by <code><a href="../reference/ivreg.html">ivreg()</a></code>.</p>
</div>
<div class="section level2">
<h2 id="review-of-2sls-estimation">Review of 2SLS Estimation<a class="anchor" aria-label="anchor" href="#review-of-2sls-estimation"></a>
</h2>
<p>We’ll need some basic results for 2SLS regression to develop
diagnostics and so we review the method briefly here. 2SLS regression
was invented independently in the 1950s by <span class="citation">Basmann (1957)</span> and Theil <span class="citation">(as cited in Theil 1971)</span>, who took slightly
different but equivalent approaches, both described below, to derive the
2SLS estimator.</p>
<p>We want to estimate the linear model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>X</mi><mi>β</mi><mo>+</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">y = X \beta + \varepsilon</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
vector of observations on a response variable,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n \times p</annotation></semantics></math>
matrix of regressors, typically with an initial columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>s
for the regression constant,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p \times 1</annotation></semantics></math>
vector of regression coefficients to be estimated from the data, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
vector of errors assumed to be distributed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msub><mi>I</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N_n(0, \sigma^2 I_n)</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>N</mi><mi>n</mi></msub><annotation encoding="application/x-tex">N_n</annotation></semantics></math>
is the multivariate-normal distribution,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
vector of zeroes, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>I</mi><mi>n</mi></msub><annotation encoding="application/x-tex">I_n</annotation></semantics></math>
is the
order-<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
identity matrix. Suppose that some (perhaps all) of the regressors in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
are <em>endogenous</em>, in the sense that they are thought not to be
independent of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>.
As a consequence, the <em>ordinary least-squares (OLS)</em> estimator
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi mathvariant="normal">O</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>⊤</mi></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">b_{\mathrm{OLS}} = (X^\top X)^{-1} X^\top y</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
is generally biased and inconsistent.</p>
<p>Now suppose that we have another set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math><em>instrumental variables (IVs)</em>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>
that are independent of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ϵ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>≥</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">q \ge p</annotation></semantics></math>.
If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">q = p</annotation></semantics></math>
we can apply the IVs directly to estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>,
but if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>&gt;</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">q &gt; p</annotation></semantics></math>
we have more IVs than we need. Simply discarding IVs would be
inefficient, and 2SLS regression is a procedure for reducing the number
of IVs to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>
by combining them in a sensible way.</p>
<p>The <em>first stage</em> of 2SLS regresses all of regressors in the
model matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
on the IVs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>
by multivariate ordinary least squares, obtaining the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>×</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">q \times p</annotation></semantics></math>
matrix of regression coefficients
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>X</mi></mrow><annotation encoding="application/x-tex">B = (Z^\top Z)^{-1} Z^\top X</annotation></semantics></math>,
and the fitted values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>X</mi><mo accent="true">̂</mo></mover><mo>=</mo><mi>Z</mi><mi>B</mi></mrow><annotation encoding="application/x-tex">\widehat{X} = Z B</annotation></semantics></math>.
The columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>
are equivalent to the coefficients produced by separate least-squares
regressions of each of the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>.
If some of the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
are exogenous, then these columns also appear in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>,
and consequently the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{X}</annotation></semantics></math>
pertaining to exogenous regressors simply reproduce the corresponding
columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.</p>
<p>Because the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{X}</annotation></semantics></math>
are linear combinations of the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>,
they are (asymptotically) uncorrelated with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>,
making them suitable IVs for estimating the regression equation. This IV
step is the <em>second stage</em> of 2SLS in Theil’s approach.</p>
<p>As an alternative, we can obtain exactly the same estimates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><annotation encoding="application/x-tex">b_{\mathrm{2SLS}}</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>
by performing an OLS regression of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{X}</annotation></semantics></math>,
producing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">b_{\mathrm{2SLS}} = (\widehat{X}^\top \widehat{X})^{-1} \widehat{X}^\top y</annotation></semantics></math>.
This is Basmann’s approach and it motivates the name “2SLS.”</p>
<p>Whether we think of the second stage as IV estimation or OLS
regression, we can combine the two stages into a single formula:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>⊤</mi></msup><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">
b_{\mathrm{2SLS}} = [X^\top Z(Z^\top Z)^{-1} Z^\top X]^{-1} X^\top Z (Z^\top Z)^{-1} Z^\top y
</annotation></semantics></math> This is what the <code>tsls()</code>
function in the <strong>sem</strong> package <span class="citation">(Fox, Nie, and Byrnes 2020)</span> does, but from the
point of view of developing regression diagnostics, it’s advantageous to
compute the 2SLS estimator by two distinct OLS regressions, which is the
approach taken by <code><a href="../reference/ivreg.html">ivreg()</a></code>.</p>
</div>
<div class="section level2">
<h2 id="unusual-data-diagnostics-for-2sls-regression">Unusual-Data Diagnostics for 2SLS Regression<a class="anchor" aria-label="anchor" href="#unusual-data-diagnostics-for-2sls-regression"></a>
</h2>
<p>As far as we can tell, diagnostics for regression models fit by 2SLS
are a relatively neglected topic, but were addressed briefly by <span class="citation">Belsley, Kuh, and Welsch (1980, 266–68)</span>.
Deletion diagnostics directly assess the influence of each case on a
fitted regression model by removing the case, refitting the model, and
noting how the regression coefficients or other regression outputs, such
as the residual standard deviation, change.</p>
<p>Case-deletion diagnostics for influential data can always be obtained
by brute-force computation, literally refitting the model with each case
removed in turn, but this approach is inefficient and consequently
unattractive in large samples. For some classes of statistical models,
such as generalized linear models <span class="citation">(e.g., Pregibon
1981)</span>, computationally less demanding approximations to
case-deletion diagnostics are available, and for linear models efficient
“updating” formulas are available <span class="citation">(as described,
e.g., by Belsley, Kuh, and Welsch 1980)</span> that permit the exact
computation of case-deletion diagnostics.</p>
<p>As it turns out, and as Belsley, Kuh, and Welsch note, exact updating
formulas for 2SLS regression permitting the efficient computation of
case-deletion statistics were given by <span class="citation">Phillips
(1977, Equations 15 and 16)</span>. Phillips’s formulas, reproduced here
in our notation (and fixing a couple of small typos in the original),
are used in the case-deletion statistics computed in the
<strong>ivreg</strong> package:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow><mo>−</mo><mi>i</mi></mrow></msub><mo>=</mo><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo>+</mo><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>g</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
b_{\mathrm{2SLS}-i} = b_{\mathrm{2SLS}} +A^{-1}g_i
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mrow><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">b_{\mathrm{2SLS}-i}</annotation></semantics></math>
is the 2SLS vector of regression coefficients with the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th
case removed, and</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>A</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>X</mi></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>g</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>u</mi><mi>i</mi></msub><mrow><mo stretchy="true" form="prefix">[</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>z</mi><mi>i</mi><mi>⊤</mi></msubsup><mi>a</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>r</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>u</mi><mi>i</mi></msub><mo>+</mo><msub><mi>j</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>u</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><mn>1</mn><mo>−</mo><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>x</mi><mi>i</mi></msub></mrow><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mn>1</mn><mo>−</mo><msub><mi>c</mi><mi>i</mi></msub><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><msub><mi>D</mi><mi>i</mi></msub></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mfrac><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>x</mi><mi>i</mi></msub></mrow><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mn>1</mn><mo>−</mo><msub><mi>c</mi><mi>i</mi></msub><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><msub><mi>D</mi><mi>i</mi></msub></mrow></mfrac><msub><mi>x</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>j</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mfrac><mrow><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>x</mi><mi>i</mi></msub></mrow><mrow><mrow><mo stretchy="true" form="prefix">[</mo><mn>1</mn><mo>−</mo><msub><mi>c</mi><mi>i</mi></msub><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><msub><mi>D</mi><mi>i</mi></msub></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>−</mo><mfrac><mn>1</mn><msub><mi>D</mi><mi>i</mi></msub></mfrac><msub><mi>x</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>r</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>z</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>c</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msubsup><mi>z</mi><mi>i</mi><mi>⊤</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>z</mi><mi>i</mi></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>D</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>1</mn><mo>−</mo><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup><mrow><mn>1</mn><mo>−</mo><msub><mi>c</mi><mi>i</mi></msub><mo>+</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>⊤</mi></msup><msup><mi>A</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>a</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>y</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
A   &amp;= X^\top Z(Z^\top Z)^{-1}Z^\top X \\
g_i &amp;= u_i[(y_i - z_i^\top a) - (y_i - r_i^\top b_{\mathrm{2SLS}})] + (u_i + j_i)(y_i - x_i^\top b_{\mathrm{2SLS}}) \\
u_i &amp;= \frac{1 - x_i^\top A^{-1} x_i}{[1 - c_i + (x_i - r_i)^\top A^{-1} (x_i - r_i)]D_i}(x_i - r_i) 
       + \frac{(x_i - r_i)^\top A^{-1} x_i}{[1 - c_i + (x_i - r_i)^\top A^{-1} (x_i - r_i)]D_i}x_i \\
j_i &amp;= \frac{(x_i - r_i)^\top A^{-1} x_i}{[1 - c_i + (x_i - r_i)^\top A^{-1} (x_i - r_i)]D_i}(x_i - r_i) - \frac{1}{D_i}x_i \\
r_i &amp;= X^\top Z (Z^\top Z)^{-1} z_i \\
c_i &amp;= z_i^\top (Z^\top Z)^{-1} z_i \\
D_i &amp;= 1 - x_i^\top A^{-1} x_i + \frac{[(x_i - r_i)^\top A^{-1} x_i]^2}{1 - c_i + (x_i - r_i)^\top A^{-1}(x_i - r_i)} \\
a   &amp;= (Z^\top Z)^{-1} Z^\top y
\end{align}</annotation></semantics></math></p>
<p>Here,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
is the value of the response for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th
case,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><annotation encoding="application/x-tex">x_i^\top</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th
row of the model matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>z</mi><mi>i</mi><mi>⊤</mi></msubsup><annotation encoding="application/x-tex">z_i^\top</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th
row of the instrumental-variables model matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>.</p>
<p>Belsley, Kuh, and Welsch specifically examine (in our notation) the
values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi></mrow><mi>i</mi></msub><mo>=</mo><msub><mi>b</mi><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo>−</mo><msub><mi>b</mi><mrow><mrow><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow><mo>−</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathrm{dfbeta}_i = b_{\mathrm{2SLS}} - b_{\mathrm{2SLS}-i}</annotation></semantics></math>.
They discuss as well the deleted values of the residual standard
deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">s_{-i}</annotation></semantics></math>.
(Belsley, Kuh, and Welsch define the residual variances
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>s</mi><mn>2</mn></msup><annotation encoding="application/x-tex">s^2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_{-i}^2</annotation></semantics></math>
respectively as the full-sample and deleted residual sums of squares
divided by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>;
in the <strong>ivreg</strong> packages, we divide by the residual
degrees of freedom,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n - p</annotation></semantics></math>
for the full-sample value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>s</mi><mn>2</mn></msup><annotation encoding="application/x-tex">s^2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mi>p</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n - p - 1</annotation></semantics></math>
for the case-deleted values.)</p>
<p>Belsley, Kuh, and Welsch then compute their summary measure of
influence on the fitted values (and regression coefficients)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow><annotation encoding="application/x-tex">\mathrm{dffits}</annotation></semantics></math>
as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow><mi>i</mi></msub><mo>=</mo><mfrac><mrow><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">t</mi><msub><mi mathvariant="normal">a</mi><mi mathvariant="normal">i</mi></msub></mrow></mrow><mrow><msub><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><msqrt><mrow><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msub><mi>x</mi><mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">
\mathrm{dffits}_i = \frac{x_i^\top \mathrm{dfbeta_{i}}}{s_{-i} \sqrt{x_i^\top (\widehat{X}^\top \widehat{X})^{-1} x_i}}
</annotation></semantics></math> where (as before)
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><annotation encoding="application/x-tex">x_i^\top</annotation></semantics></math>
is the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th
row of the model matrix
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{X}</annotation></semantics></math>
is the model matrix of second-stage regressors.</p>
<p>Let
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mo>*</mo></msup><mo>=</mo><mi>X</mi><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>⊤</mi></msup><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">
H^* = X[X^\top Z(Z^\top Z)^{-1} Z^\top X]^{-1} X^\top Z (Z^\top Z)^{-1} Z^\top
</annotation></semantics></math> represent the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
matrix that transforms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
into the fitted values,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>y</mi><mo accent="true">̂</mo></mover><mo>=</mo><msup><mi>H</mi><mo>*</mo></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">\widehat{y} = H^* y</annotation></semantics></math>.
In OLS regression, the analogous quantity is the <em>hat-matrix</em>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mi>X</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>X</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">H = X(X^\top X)^{-1}X^\top</annotation></semantics></math>.
Belsley, Kuh, and Welsch note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>H</mi><mo>*</mo></msup><annotation encoding="application/x-tex">H^*</annotation></semantics></math>,
unlike
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>,
is not an orthogonal-projection matrix, projecting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
orthogonally onto the subspace spanned by the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.
(They say that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>H</mi><mo>*</mo></msup><annotation encoding="application/x-tex">H^*</annotation></semantics></math>
isn’t a projection matrix, but that isn’t true: It represents an oblique
projection of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
onto the subspace spanned by the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.)
In particular, although
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>H</mi><mo>*</mo></msup><annotation encoding="application/x-tex">H^*</annotation></semantics></math>,
like
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>,
is idempotent
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mo>*</mo></msup><mo>=</mo><msup><mi>H</mi><mo>*</mo></msup><msup><mi>H</mi><mo>*</mo></msup></mrow><annotation encoding="application/x-tex">H^* = H^* H^*</annotation></semantics></math>)
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>H</mi><mo>*</mo></msup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">\mathrm{trace}(H^*) = p</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>H</mi><mo>*</mo></msup><annotation encoding="application/x-tex">H^*</annotation></semantics></math>,
unlike
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>,
is asymmetric, and thus its diagonal elements can’t be treated as
summary measures of leverage, that is, as <em>hatvalues</em>.</p>
<p>Belsley, Kuh, and Welsch recommend simply using the havalues from the
second-stage regression. These are the diagonal entries
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><msub><mi>h</mi><mrow><mi>i</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">h_i = h_{ii}</annotation></semantics></math>
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mn>2</mn></msub><mo>=</mo><mover><mi>X</mi><mo accent="true">̂</mo></mover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mover><mi>X</mi><mo accent="true">̂</mo></mover><mi>⊤</mi></msup></mrow><annotation encoding="application/x-tex">H_2 = \widehat{X}(\widehat{X}^\top \widehat{X})^{-1} \widehat{X}^\top</annotation></semantics></math>.
We discuss some alternatives below.</p>
<p>In addition to hatvalues,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi></mrow><annotation encoding="application/x-tex">\mathrm{dfbeta}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">s_{-i}</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow><annotation encoding="application/x-tex">\mathrm{dffits}</annotation></semantics></math>,
the <strong>ivreg</strong> packages calculates <em>Cook’s distances</em>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>D</mi><mi>i</mi></msub><annotation encoding="application/x-tex">D_i</annotation></semantics></math>,
which are essentially a slightly differently scaled version of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow><annotation encoding="application/x-tex">\mathrm{dffits}</annotation></semantics></math>
that uses the overall residual standard deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
in place of the deleted standard deviations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">s_{-i}</annotation></semantics></math>:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub><mo>=</mo><mfrac><msubsup><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow><mn>2</mn></msubsup><msup><mi>s</mi><mn>2</mn></msup></mfrac><mo>×</mo><mfrac><msubsup><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">s</mi></mrow><mi>i</mi><mn>2</mn></msubsup><mi>p</mi></mfrac></mrow><annotation encoding="application/x-tex">
D_i = \frac{s_{-i}^2}{s^2} \times \frac{\mathrm{dffits}_i^2}{p}
</annotation></semantics></math></p>
<p>Because they have equal variances and are approximately
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-distributed
under the normal linear model, <em>studentized residuals</em> are useful
for detecting outliers and for addressing the assumption of normally
distributed errors. The <strong>ivreg</strong> package defines
studentized residuals in analogy to OLS regression as
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi></mrow><mi>i</mi></msub><mo>=</mo><mfrac><msub><mi>e</mi><mi>i</mi></msub><mrow><msub><mi>s</mi><mrow><mo>−</mo><mi>i</mi></mrow></msub><msqrt><mrow><mn>1</mn><mo>−</mo><msub><mi>h</mi><mi>i</mi></msub></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">
\mathrm{rstudent}_i = \frac{e_i}{s_{-i} \sqrt{1 - h_i}}
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>x</mi><mi>i</mi><mi>⊤</mi></msubsup><msub><mi>b</mi><mrow><mn>2</mn><mi>S</mi><mi>L</mi><mi>S</mi></mrow></msub></mrow><annotation encoding="application/x-tex">e_i = y_i - x_i^\top b_{2SLS}</annotation></semantics></math>
is the <em>response residual</em> for the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>-th
case.</p>
<p>As mentioned, <span class="citation">Belsley, Kuh, and Welsch
(1980)</span> recommend using hatvalues from the second-stage
regression. That’s a reasonable choice and the default in the
<strong>ivreg</strong> package, but it risks missing cases that have
high leverage in the first-stage but not the second-stage regression.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">h_i^{(1)}</annotation></semantics></math>
represent the hatvalues from the first stage and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><annotation encoding="application/x-tex">h_i^{(2)}</annotation></semantics></math>
those from the second stage. If the model includes an intercept, both
sets of hatvalues are bounded by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">1/n</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>,
but the average hatvalue in the first stage is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">q/n</annotation></semantics></math>
while the average in the second stage is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">p/n</annotation></semantics></math>.
To make the hatvalues from the two stages comparable, we divide each by
its average,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>*</mo><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mfrac><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mrow><mi>q</mi><mi>/</mi><mi>n</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">h^{(1*)}_i = \frac{h_i^{(1)}}{q/n}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>*</mo><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>=</mo><mfrac><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mrow><mi>p</mi><mi>/</mi><mi>n</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">h^{(2*)}_i = \frac{h_i^{(2)}}{p/n}</annotation></semantics></math>.
Then we can define the two-stages hatvalue either as the (rescaled)
larger of the two for each case,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mi>/</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>*</mo><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>*</mo><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">h_i = (p/n) \times \max \left( h^{(1*)}_i, h^{(2*)}_i \right)</annotation></semantics></math>,
or as their (rescaled) geometric mean,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>p</mi><mi>/</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><msqrt><mrow><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>*</mo><mo stretchy="true" form="postfix">)</mo></mrow></msubsup><mo>×</mo><msubsup><mi>h</mi><mi>i</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>2</mn><mo>*</mo><mo stretchy="true" form="postfix">)</mo></mrow></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">h_i = (p/n) \times \sqrt{h^{(1*)}_i \times  h^{(2*)}_i}</annotation></semantics></math>.
The <strong>ivreg</strong> package provides both of these options.</p>
<div class="section level3">
<h3 id="unusual-data-diagnostics-in-the-ivreg-package">Unusual-Data Diagnostics in the <strong>ivreg</strong> Package<a class="anchor" aria-label="anchor" href="#unusual-data-diagnostics-in-the-ivreg-package"></a>
</h3>
<p>The <strong>ivreg</strong> package implements unusual-data
diagnostics for 2SLS regression (i.e., class <code>"ivreg"</code>
objects produced by <code><a href="../reference/ivreg.html">ivreg()</a></code>) as methods for various generic
functions in the <strong>stats</strong> and <strong>car</strong>
packages; these functions include <code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">cooks.distance()</a></code>,
<code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">dfbeta()</a></code>, <code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hatvalues()</a></code>,
<code><a href="https://rdrr.io/r/stats/lm.influence.html" class="external-link">influence()</a></code>, and <code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">rstudent()</a></code> in
<strong>stats</strong>, and <code><a href="https://rdrr.io/pkg/car/man/avPlots.html" class="external-link">avPlot()</a></code> and
<code><a href="https://rdrr.io/pkg/car/man/qqPlot.html" class="external-link">qqPlot()</a></code> in <strong>car</strong>. In particular,
<code><a href="../reference/ivregDiagnostics.html">influence.ivreg()</a></code> returns an object containing several
diagnostic statistics, and it is thus more efficient to use the
<code><a href="https://rdrr.io/r/stats/lm.influence.html" class="external-link">influence()</a></code> function than to compute the various
diagnostics separately. Methods provided for class
<code>"influence.ivreg"</code> objects include
<code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">cooks.distance()</a></code>, <code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">dfbeta()</a></code>,
<code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hatvalues()</a></code>, <code><a href="https://rdrr.io/pkg/car/man/qqPlot.html" class="external-link">qqPlot()</a></code>, and
<code><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">rstudent()</a></code>.</p>
<p>The package also provides methods for various standard R
regression-model generics, including <code><a href="https://rdrr.io/r/stats/anova.html" class="external-link">anova()</a></code> (for model
comparison), <code>predicted()</code> for computing predicted values,
<code><a href="https://rdrr.io/r/stats/model.matrix.html" class="external-link">model.matrix()</a></code> (for the model or for the first- or
second-stage regression), <code><a href="https://rdrr.io/r/base/print.html" class="external-link">print()</a></code>, <code><a href="https://rdrr.io/r/stats/residuals.html" class="external-link">residuals()</a></code>
(of several kinds), <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code>, <code><a href="https://rdrr.io/r/stats/update.html" class="external-link">update()</a></code>, and
<code><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov()</a></code>. The <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> method makes provision
for a user-specified coefficient covariance matrix or for a function to
compute the coefficient covariance matrix, such as
<code><a href="https://sandwich.R-Forge.R-project.org/reference/sandwich.html" class="external-link">sandwich()</a></code> in the <strong>sandwich</strong> package, to
compute robust coefficient covariances. The latter is supported by
methods for the <code><a href="https://sandwich.R-Forge.R-project.org/reference/bread.html" class="external-link">bread()</a></code> and <code><a href="https://sandwich.R-Forge.R-project.org/reference/estfun.html" class="external-link">estfun()</a></code> generics
defined in <strong>sandwich</strong>.</p>
</div>
<div class="section level3">
<h3 id="unusual-data-diagnostics-an-example">Unusual Data Diagnostics: An Example<a class="anchor" aria-label="anchor" href="#unusual-data-diagnostics-an-example"></a>
</h3>
<p>The <strong>ivreg</strong> package contains the <code>Kmenta</code>
data set, used in <span class="citation">Kmenta (1986, Ch. 13)</span> to
illustrate estimation (by 2SLS and other methods) of a linear
simultaneous equation econometric model. The data, which are partly
contrived, represent an annual time series for the U.S. economy from
1922 to 1941, with the following variables:</p>
<ul>
<li>
<code>Q</code>, food consumption per capita</li>
<li>
<code>P</code>, ratio of food prices to general consumer prices</li>
<li>
<code>D</code>, disposable income in constant dollars</li>
<li>
<code>F</code>, ratio of preceding year’s prices received by farmers
to general consumer prices</li>
<li>
<code>A</code>, time in years</li>
</ul>
<p>The data set is small and so we can examine it in its entirety:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://zeileis.github.io/ivreg/">"ivreg"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Kmenta"</span>, package <span class="op">=</span> <span class="st">"ivreg"</span><span class="op">)</span></span>
<span><span class="va">Kmenta</span></span></code></pre></div>
<pre><code><span><span class="co">##            Q       P     D     F  A</span></span>
<span><span class="co">## 1922  98.485 100.323  87.4  98.0  1</span></span>
<span><span class="co">## 1923  99.187 104.264  97.6  99.1  2</span></span>
<span><span class="co">## 1924 102.163 103.435  96.7  99.1  3</span></span>
<span><span class="co">## 1925 101.504 104.506  98.2  98.1  4</span></span>
<span><span class="co">## 1926 104.240  98.001  99.8 110.8  5</span></span>
<span><span class="co">## 1927 103.243  99.456 100.5 108.2  6</span></span>
<span><span class="co">## 1928 103.993 101.066 103.2 105.6  7</span></span>
<span><span class="co">## 1929  99.900 104.763 107.8 109.8  8</span></span>
<span><span class="co">## 1930 100.350  96.446  96.6 108.7  9</span></span>
<span><span class="co">## 1931 102.820  91.228  88.9 100.6 10</span></span>
<span><span class="co">## 1932  95.435  93.085  75.1  81.0 11</span></span>
<span><span class="co">## 1933  92.424  98.801  76.9  68.6 12</span></span>
<span><span class="co">## 1934  94.535 102.908  84.6  70.9 13</span></span>
<span><span class="co">## 1935  98.757  98.756  90.6  81.4 14</span></span>
<span><span class="co">## 1936 105.797  95.119 103.1 102.3 15</span></span>
<span><span class="co">## 1937 100.225  98.451 105.1 105.0 16</span></span>
<span><span class="co">## 1938 103.522  86.498  96.4 110.5 17</span></span>
<span><span class="co">## 1939  99.929 104.016 104.4  92.5 18</span></span>
<span><span class="co">## 1940 105.223 105.769 110.7  89.3 19</span></span>
<span><span class="co">## 1941 106.232 113.490 127.1  93.0 20</span></span></code></pre>
<p>Kmenta estimated the following two-equation model, with the first
equation representing demand and the second supply:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>Q</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>β</mi><mn>10</mn></msub><mo>+</mo><msub><mi>β</mi><mn>11</mn></msub><mi>P</mi><mo>+</mo><msub><mi>β</mi><mn>12</mn></msub><mi>D</mi><mo>+</mo><msub><mi>ε</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>Q</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><msub><mi>β</mi><mn>20</mn></msub><mo>+</mo><msub><mi>β</mi><mn>21</mn></msub><mi>P</mi><mo>+</mo><msub><mi>β</mi><mn>22</mn></msub><mi>F</mi><mo>+</mo><msub><mi>β</mi><mn>23</mn></msub><mi>A</mi><mo>+</mo><msub><mi>ε</mi><mn>2</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
Q &amp;= \beta_{10} + \beta_{11} P + \beta_{12} D + \varepsilon_1 \\
Q &amp;= \beta_{20} + \beta_{21} P + \beta_{22} F + \beta_{23} A + \varepsilon_2
\end{align}</annotation></semantics></math> The variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
are taken as exogenous, as of course is the constant regressor (a
columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>s),
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
in both structural equations is an endogenous explanatory variable.
Because there are four instrumental variables available, the first
structural equation, which has three coefficients, is over-identified,
while the second structural equation, with four coefficients, is
just-identified.</p>
<p>The values of the exogenous variables are real, while those of the
endogenous variables were generated (i.e., simulated) by Kmenta
according to the model, with the following assumed values of the
parameters:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>Q</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>96.5</mn><mo>−</mo><mn>0.25</mn><mi>P</mi><mo>+</mo><mn>0.30</mn><mi>D</mi><mo>+</mo><msub><mi>ε</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>Q</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>62.5</mn><mo>+</mo><mn>0.15</mn><mi>P</mi><mo>+</mo><mn>0.20</mn><mi>F</mi><mo>+</mo><mn>0.36</mn><mi>A</mi><mo>+</mo><msub><mi>ε</mi><mn>2</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
Q &amp;= 96.5 - 0.25 P + 0.30 D + \varepsilon_1 \\
Q &amp;= 62.5 + 0.15 P + 0.20 F + 0.36 A + \varepsilon_2
\end{align}</annotation></semantics></math></p>
<p>Solving the structural equations for the endogenous variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
produces the <em>reduced form</em> of the model</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>Q</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>75.25</mn><mo>+</mo><mn>11.25</mn><mi>D</mi><mo>+</mo><mn>0.125</mn><mi>F</mi><mo>+</mo><mn>0.225</mn><mi>A</mi><mo>+</mo><msub><mi>ν</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>P</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mn>85.00</mn><mo>+</mo><mn>0.75</mn><mi>D</mi><mo>−</mo><mn>0.50</mn><mi>F</mi><mo>−</mo><mn>0.90</mn><mi>A</mi><mo>+</mo><msub><mi>ν</mi><mn>2</mn></msub></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{align}
Q &amp;= 75.25 + 11.25 D + 0.125 F + 0.225 A + \nu_1\\
P &amp;= 85.00 + 0.75 D - 0.50 F - 0.90 A + \nu_2
\end{align}</annotation></semantics></math></p>
<p>Kmenta independently sampled 20 values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>δ</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\delta_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>δ</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\delta_2</annotation></semantics></math>,
each from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">N(0, 1)</annotation></semantics></math>,
and then set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ν</mi><mn>1</mn></msub><mo>=</mo><mn>2</mn><msub><mi>δ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\nu_1 = 2 \delta_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ν</mi><mn>2</mn></msub><mo>=</mo><mo>−</mo><mn>0.5</mn><msub><mi>ν</mi><mn>1</mn></msub><mo>+</mo><msub><mi>δ</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\nu_2 = -0.5 \nu_1 + \delta_2</annotation></semantics></math>.</p>
<p>The structural equations are estimated as follows by the
<code><a href="../reference/ivreg.html">ivreg()</a></code> function <span class="citation">(compare Kmenta
1986, 686)</span>:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deq</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Q</span> <span class="op">~</span> <span class="va">P</span> <span class="op">+</span> <span class="va">D</span> <span class="op">|</span> <span class="va">D</span> <span class="op">+</span> <span class="cn">F</span> <span class="op">+</span> <span class="va">A</span>, data<span class="op">=</span><span class="va">Kmenta</span><span class="op">)</span>     <span class="co"># demand equation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">## -3.4305 -1.2432 -0.1895  1.5762  2.4920 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 94.63330    7.92084  11.947 1.08e-09 ***</span></span>
<span><span class="co">## P           -0.24356    0.09648  -2.524   0.0218 *  </span></span>
<span><span class="co">## D            0.31399    0.04694   6.689 3.81e-06 ***</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   2  16    88.025 2.32e-09 ***</span></span>
<span><span class="co">## Wu-Hausman         1  16    11.422  0.00382 ** </span></span>
<span><span class="co">## Sargan             1  NA     2.983  0.08414 .  </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 1.966 on 17 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.7548,  Adjusted R-squared: 0.726 </span></span>
<span><span class="co">## Wald test: 23.81 on 2 and 17 DF,  p-value: 1.178e-05</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">seq</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/ivreg.html">ivreg</a></span><span class="op">(</span><span class="va">Q</span> <span class="op">~</span> <span class="va">P</span> <span class="op">+</span> <span class="cn">F</span> <span class="op">+</span> <span class="va">A</span> <span class="op">|</span> <span class="va">D</span> <span class="op">+</span> <span class="cn">F</span> <span class="op">+</span> <span class="va">A</span>, data<span class="op">=</span><span class="va">Kmenta</span><span class="op">)</span> <span class="co"># supply equation</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">seq</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + F + A | D + F + A, data = Kmenta)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">## -4.8724 -1.2593  0.6415  1.4745  3.4865 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 49.53244   12.01053   4.124 0.000795 ***</span></span>
<span><span class="co">## P            0.24008    0.09993   2.402 0.028785 *  </span></span>
<span><span class="co">## F            0.25561    0.04725   5.410 5.79e-05 ***</span></span>
<span><span class="co">## A            0.25292    0.09966   2.538 0.021929 *  </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   1  16    256.34 2.86e-11 ***</span></span>
<span><span class="co">## Wu-Hausman         1  15     36.14 2.38e-05 ***</span></span>
<span><span class="co">## Sargan             0  NA        NA       NA    </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 2.458 on 16 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.6396,  Adjusted R-squared: 0.572 </span></span>
<span><span class="co">## Wald test:  10.7 on 3 and 16 DF,  p-value: 0.0004196</span></span></code></pre>
<p>By default, <code><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary()</a></code> prints the results of three
“diagnostic” tests for 2SLS regression <span class="citation">(see e.g.,
Greene 2003)</span>. These tests (which can be suppressed by setting the
argument <code>diagnostics=FALSE</code>) are not the focus of the
vignette and so we’ll comment on them only briefly:</p>
<ul>
<li><p>A good instrumental variable is highly correlated with one or
more of the explanatory variables while remaining uncorrelated with the
errors. If an endogenous regressor is only weakly related to the
instrumental variables, then its coefficient will be estimated
imprecisely. We hope for a large test statistic and small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value
in the diagnostic test for weak instruments, as is the case for both
regression equations in the Kmenta model.</p></li>
<li><p>Applied to 2SLS regression, the Wu–Hausman test is a test of
<em>endogeneity</em>. If all of the regressors are exogenous, then both
the OLS and 2SLS estimators are consistent, and the OLS estimator is
more efficient, but if one or more regressors are endogenous, then the
OLS estimator is inconsistent. A large test statistic and small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value,
as in the example, suggests that the OLS estimator is inconsistent and
the 2SLS estimator is therefore to be preferred.</p></li>
<li><p>The Sargan test is a test of overidentification. That is, in an
overidentified regression equation, where there are more instrumental
variables than coefficients to estimate, as in Kmenta’s demand equation,
it’s possible that the instrumental variables provide conflicting
information about the values of the coefficients. A large test statistic
and small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value
for the Sargan test suggest, therefore, that the model is misspecified.
In the example, we obtain a moderately small
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-value
of 0.084 by chance even though we know (by the manner in which the data
were constructed) that the demand equation is correct. The Sargan test
is inapplicable to a just-identified regression equation, with an equal
number of instrumental variables and coefficients, as in Kmenta’s supply
equation.</p></li>
</ul>
<p>Several methods for class <code>"lm"</code> objects work properly
with the objects produced by <code><a href="../reference/ivreg.html">ivreg()</a></code>. For example, the
<code><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot()</a></code> method for <code>"ivreg"</code> objects invokes the
corresponding <code>"lm"</code> method and produces interpretable plots,
here for the 2SLS fit for the demand equation in Kmenta’s model:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/par.html" class="external-link">par</a></span><span class="op">(</span>mfrow<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-4-1.png" width="768"></p>
<p>In this case, however, we prefer the versions of these diagnostic
graphs described below, in this and subsequent sections.</p>
<p>As we mentioned, the <code>Kmenta</code> data are partly contrived by
simulating the model, and so it’s probably not surprising that the data
are well behaved. For example, a <em>QQ plot</em> of studentized
residuals and an <em>“influence plot”</em> of hatvalues, studentized
residuals, and Cook’s distances for the first structural equation are
both unremarkable, except for a couple of high-leverage but in-line
cases:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://r-forge.r-project.org/projects/car/" class="external-link">"car"</a></span><span class="op">)</span> <span class="co"># for diagnostic generic functions</span></span></code></pre></div>
<pre><code><span><span class="co">## Loading required package: carData</span></span></code></pre>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/qqPlot.html" class="external-link">qqPlot</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-5-1.png" width="384"></p>
<pre><code><span><span class="co">## 1937 1929 </span></span>
<span><span class="co">##   16    8</span></span></code></pre>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/influencePlot.html" class="external-link">influencePlot</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-6-1.png" width="384"></p>
<pre><code><span><span class="co">##      StudRes        Hat      CookD</span></span>
<span><span class="co">## 1 -1.7359357 0.09079703 0.06956671</span></span>
<span><span class="co">## 2 -1.3686682 0.26453459 0.21973049</span></span>
<span><span class="co">## 3 -2.0995532 0.13849570 0.17147564</span></span>
<span><span class="co">## 4 -0.2010944 0.39711512 0.01508349</span></span>
<span><span class="co">## 5 -0.4505155 0.46498004 0.05257374</span></span></code></pre>
<p>The circles in the influence plot have areas proportional to Cook’s
D, the horizontal lines are drawn at 0 and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>±</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\pm 2</annotation></semantics></math>
on the studentized residuals scale (the horizontal line at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">t</mi></mrow><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\mathrm{rstudent} = 2</annotation></semantics></math>
is off the graph), and the vertical lines are at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>×</mo><mover><mi>h</mi><mo accent="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">2 \times \bar{h}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>×</mo><mover><mi>h</mi><mo accent="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">3 \times \bar{h}</annotation></semantics></math>.
We invite the reader to repeat these graphs, and the example below, for
the second structural equation.</p>
<p>To generate a more interesting example, we’ll change the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
for the high-leverage 20th case (i.e, for 1941) from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mn>20</mn></msub><mo>=</mo><mn>106.232</mn></mrow><annotation encoding="application/x-tex">Q_{20} = 106.232</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mn>20</mn></msub><mo>=</mo><mn>95</mn></mrow><annotation encoding="application/x-tex">Q_{20} = 95</annotation></semantics></math>,
a value that’s well within the range of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
in the data but out of line with the rest of the data:</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Kmenta1</span> <span class="op">&lt;-</span> <span class="va">Kmenta</span></span>
<span><span class="va">Kmenta1</span><span class="op">[</span><span class="fl">20</span>, <span class="st">"Q"</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">95</span></span></code></pre></div>
<p>Then repeating the 2SLS fit for the first structural equation and
comparing the results to those for the uncorrupted data reveals
substantial change in the regression coefficients:</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deq1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">deq</span>, data<span class="op">=</span><span class="va">Kmenta1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/compareCoefs.html" class="external-link">compareCoefs</a></span><span class="op">(</span><span class="va">deq</span>, <span class="va">deq1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Calls:</span></span>
<span><span class="co">## 1: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)</span></span>
<span><span class="co">## 2: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##             Model 1 Model 2</span></span>
<span><span class="co">## (Intercept)   94.63  117.96</span></span>
<span><span class="co">## SE             7.92   11.64</span></span>
<span><span class="co">##                            </span></span>
<span><span class="co">## P           -0.2436 -0.4054</span></span>
<span><span class="co">## SE           0.0965  0.1417</span></span>
<span><span class="co">##                            </span></span>
<span><span class="co">## D            0.3140  0.2351</span></span>
<span><span class="co">## SE           0.0469  0.0690</span></span>
<span><span class="co">## </span></span></code></pre>
<p>The problematic 20th case (the year 1941) is clearly revealed by
unusual-data regression diagnostics:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/qqPlot.html" class="external-link">qqPlot</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-9-1.png" width="384"></p>
<pre><code><span><span class="co">## 1941 1940 </span></span>
<span><span class="co">##   20   19</span></span></code></pre>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/outlierTest.html" class="external-link">outlierTest</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##       rstudent unadjusted p-value Bonferroni p</span></span>
<span><span class="co">## 1941 -4.599583         0.00029602    0.0059204</span></span></code></pre>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/influencePlot.html" class="external-link">influencePlot</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-11-1.png" width="384"></p>
<pre><code><span><span class="co">##      StudRes       Hat     CookD</span></span>
<span><span class="co">## 1 -1.4737565 0.2645346 0.2447875</span></span>
<span><span class="co">## 2 -0.9139638 0.3971151 0.2269833</span></span>
<span><span class="co">## 3  1.6021281 0.1280020 0.1155278</span></span>
<span><span class="co">## 4 -4.5995825 0.4649800 2.8361307</span></span></code></pre>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/avPlots.html" class="external-link">avPlots</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-12-1.png" width="768"></p>
<p>Removing the 20th case produces estimated coefficients close to those
for the uncorrupted data:</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deq1.20</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">deq1</span>, subset <span class="op">=</span> <span class="op">-</span><span class="fl">20</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/compareCoefs.html" class="external-link">compareCoefs</a></span><span class="op">(</span><span class="va">deq</span>, <span class="va">deq1</span>, <span class="va">deq1.20</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Calls:</span></span>
<span><span class="co">## 1: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)</span></span>
<span><span class="co">## 2: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1)</span></span>
<span><span class="co">## 3: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1, subset = -20)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##             Model 1 Model 2 Model 3</span></span>
<span><span class="co">## (Intercept)   94.63  117.96   92.42</span></span>
<span><span class="co">## SE             7.92   11.64    9.67</span></span>
<span><span class="co">##                                    </span></span>
<span><span class="co">## P           -0.2436 -0.4054 -0.2300</span></span>
<span><span class="co">## SE           0.0965  0.1417  0.1047</span></span>
<span><span class="co">##                                    </span></span>
<span><span class="co">## D            0.3140  0.2351  0.3233</span></span>
<span><span class="co">## SE           0.0469  0.0690  0.0527</span></span>
<span><span class="co">## </span></span></code></pre>
<p>The standard errors of the estimated coefficients are larger than
they were originally because we now have 19 rather than 20 cases and
because the variation of the explanatory variables is reduced.</p>
<p>It’s of some interest to discover whether the three definitions of
hatvaues make a practical difference to this example. A scatterplot
matrix for the three kinds of hatvalues suggests that they all produce
similar results:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">H</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hatvalues</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hatvalues</a></span><span class="op">(</span><span class="va">deq1</span>, type<span class="op">=</span><span class="st">"both"</span><span class="op">)</span>, </span>
<span>           <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hatvalues</a></span><span class="op">(</span><span class="va">deq1</span>, type<span class="op">=</span><span class="st">"maximum"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"stage2"</span>, <span class="st">"geom.mean"</span>, <span class="st">"maximum"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">H</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##          stage2  geom.mean    maximum</span></span>
<span><span class="co">## 1922 0.10349313 0.12269459 0.14545857</span></span>
<span><span class="co">## 1923 0.11215042 0.12972476 0.15005306</span></span>
<span><span class="co">## 1924 0.08882553 0.10233878 0.11790784</span></span>
<span><span class="co">## 1925 0.09515432 0.10207539 0.10949987</span></span>
<span><span class="co">## 1926 0.06166289 0.07959715 0.10274748</span></span>
<span><span class="co">## 1927 0.05684346 0.06794727 0.08122009</span></span></code></pre>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html" class="external-link">scatterplotMatrix</a></span><span class="op">(</span><span class="va">H</span>, smooth<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-14-1.png" width="576"></p>
<p>Finally, let’s verify that the deletion diagnostics are correctly
computed:</p>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">dfbeta</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span><span class="op">[</span><span class="fl">20</span>, <span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/stats/coef.html" class="external-link">coef</a></span><span class="op">(</span><span class="va">deq1.20</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##                    [,1]        [,2]</span></span>
<span><span class="co">## (Intercept) 25.53936742 25.53936742</span></span>
<span><span class="co">## P           -0.17547231 -0.17547231</span></span>
<span><span class="co">## D           -0.08827334 -0.08827334</span></span></code></pre>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.influence.html" class="external-link">influence</a></span><span class="op">(</span><span class="va">deq1</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span><span class="op">[</span><span class="fl">20</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html" class="external-link">sigma</a></span><span class="op">(</span><span class="va">deq1.20</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##     1941          </span></span>
<span><span class="co">## 2.028434 2.028434</span></span></code></pre>
</div>
</div>
<div class="section level2">
<h2 id="nonlinearity-diagnostics">Nonlinearity Diagnostics<a class="anchor" aria-label="anchor" href="#nonlinearity-diagnostics"></a>
</h2>
<p>The theoretical properties of <em>component-plus-residual plots</em>
as nonlinearity diagnostics were systematically explored by <span class="citation">Cook (1993)</span> and <span class="citation">Cook and
Croos-Dabrera (1998)</span>. Following these authors and focusing on the
explanatory variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>,
let’s assume that the partial relationship of the response
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>
is potentially nonlinear, as represented by the partial regression
function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f(x_1)</annotation></semantics></math>,
and that the partial relationships of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
to the other
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>s
are linear, so that an accurate model for the data is:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>α</mi><mo>+</mo><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>β</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><msub><mi>β</mi><mi>k</mi></msub><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">
E(y) = \alpha + f(x_1) + \beta_2 x_2 + \cdots + \beta_k x_k 
</annotation></semantics></math></p>
<p>We don’t know
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f()</annotation></semantics></math>
and so instead fit the <em>working model</em>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>α</mi><mi>′</mi></msup><mo>+</mo><msubsup><mi>β</mi><mn>1</mn><mi>′</mi></msubsup><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>β</mi><mn>2</mn><mi>′</mi></msubsup><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><msubsup><mi>β</mi><mi>k</mi><mi>′</mi></msubsup><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">
E(y) = \alpha^\prime + \beta_1^\prime x_1 + \beta_2^\prime x_2 + \cdots + \beta_k^\prime x_k 
</annotation></semantics></math> in our case by 2SLS regression,
obtaining estimated regression coefficients
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>a</mi><mi>′</mi></msup><mo>,</mo><msubsup><mi>b</mi><mn>1</mn><mi>′</mi></msubsup><mo>,</mo><msubsup><mi>b</mi><mn>2</mn><mi>′</mi></msubsup><mo>,</mo><mi>…</mi><mo>,</mo><msubsup><mi>b</mi><mi>k</mi><mi>′</mi></msubsup></mrow><annotation encoding="application/x-tex">a^\prime, b_1^\prime, b_2^\prime, \ldots, b_k^\prime</annotation></semantics></math>.
Cook and Croos-Dabrera’s work shows that as long as the regression
estimator is consistent and the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>s
are linearly related, the partial residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>b</mi><mn>1</mn><mi>′</mi></msubsup><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mi>e</mi></mrow><annotation encoding="application/x-tex">b^\prime_1 x_1 + e</annotation></semantics></math>
can be plotted and smoothed against
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>
to visualize an estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f()</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>e</mi><mo>=</mo><mi>y</mi><mo>−</mo><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>a</mi><mi>′</mi></msup><mo>+</mo><msubsup><mi>b</mi><mn>1</mn><mi>′</mi></msubsup><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msubsup><mi>b</mi><mn>2</mn><mi>′</mi></msubsup><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>⋯</mi><msubsup><mi>b</mi><mi>k</mi><mi>′</mi></msubsup><msub><mi>x</mi><mi>k</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">e = y - (a^\prime + b_1^\prime x_1 + b_2^\prime x_2 + \cdots b_k^\prime x_k)</annotation></semantics></math>
are the response residuals. In practice, the component-plus-residual
plot can break down as an accurate representation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">f()</annotation></semantics></math>
if there are strong nonlinear relationships between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>
and the other
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>s
or if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is nonlinearly related to another
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
that is correlated with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>.</p>
<p><span class="citation">Fox and Weisberg (2018)</span> extend
component-plus-residual plots to more complex regression models, which
can, for example, include interactions, by adding partial residuals to
<em>predictor effect plots</em>. These graphs also can be applied to
linear models fit by 2SLS regression.</p>
<div class="section level3">
<h3 id="diagnosing-nonlinearity-an-example">Diagnosing Nonlinearity: An Example<a class="anchor" aria-label="anchor" href="#diagnosing-nonlinearity-an-example"></a>
</h3>
<p>We turn once more to the demand equation for Kmenta’s data and model
to illustrate component-plus-residual plots, and once more the data are
well behaved. An <code>"ivreg"</code> method is provided for the
<code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlot()</a></code> function in the <strong>car</strong> package. In
particular, <code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots()</a></code> constructs component-plus-residual
plots for all of the numeric explanatory variables in an additive
regression equation. For example,</p>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">deq</span>, smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>span<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-17-1.png" width="768"></p>
<p>We set a large <em>span</em> for the <em>loess smoother</em> <span class="citation">(Cleveland, Grosse, and Shyu 1992)</span> in the plot
because there are only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">n = 20</annotation></semantics></math>
cases in the <code>Kmenta</code> data set. The default value of the span
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>/</mi><mn>3</mn></mrow><annotation encoding="application/x-tex">2/3</annotation></semantics></math>.
In each panel, the loess smooth, given by the magenta line, closely
matches the least-squares line, given by the broken blue line, which
represents the fitted regression plane viewed edge-on in the direction
of the <em>focal explanatory variable</em>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
on the left and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
on the right. Both partial relationships therefore appear to be
linear.</p>
<p>CERES plots <span class="citation">(Cook 1993)</span>, implemented in
the <code><a href="https://rdrr.io/pkg/car/man/ceresPlots.html" class="external-link">ceresPlots()</a></code> function in the <strong>car</strong>
package, are a version of component-plus-residuals plots that use
smoothers rather than linear regression and that therefore are more
robust with respect to nonlinear relationships among the predictors. In
most applications, component-plus-residuals and CERES plots produce
similar results, and that’s the case here:</p>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/ceresPlots.html" class="external-link">ceresPlots</a></span><span class="op">(</span><span class="va">deq</span>, smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>span<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-18-1.png" width="768"></p>
<p><code><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots()</a></code> and <code><a href="https://rdrr.io/pkg/car/man/ceresPlots.html" class="external-link">ceresPlots()</a></code> work only for
additive models; the <code><a href="https://rdrr.io/pkg/effects/man/predictorEffects.html" class="external-link">predictorEffects()</a></code> function in the
<strong>effects</strong> package plots partial residuals for more
complex models. In the current example, which is an additive model, we
get essentially the same graphs as before, except for the scaling of the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
axis:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://www.r-project.org" class="external-link">"effects"</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## lattice theme set by effectsTheme()</span></span>
<span><span class="co">## See ?effectsTheme for details.</span></span></code></pre>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/effects/man/predictorEffects.html" class="external-link">predictorEffects</a></span><span class="op">(</span><span class="va">deq</span>, residuals<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>, </span>
<span>     partial.residuals<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>span<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-19-1.png" width="768"></p>
<p>The shaded blue regions in the predictor effect plots represent
pointwise 95% confidence envelopes around the fitted partial-regression
lines.</p>
<p>Suppose, however, that we fit the wrong model to the data:</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deq2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">deq</span>, <span class="va">.</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html" class="external-link">I</a></span><span class="op">(</span><span class="op">(</span><span class="va">P</span> <span class="op">-</span> <span class="fl">85</span><span class="op">)</span><span class="op">^</span><span class="fl">4</span><span class="op">/</span><span class="fl">10</span><span class="op">^</span><span class="fl">5</span><span class="op">)</span> <span class="op">+</span> <span class="va">D</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html" class="external-link">crPlots</a></span><span class="op">(</span><span class="va">deq2</span>, smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>span<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-20-1.png" width="768"></p>
<p>Because the ratio
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>113.49</mn><mi>/</mi><mn>86.50</mn><mo>=</mo><mn>1.3</mn></mrow><annotation encoding="application/x-tex">\max(P)/\min(P) = 113.49/86.50 = 1.3</annotation></semantics></math>
is not much larger than 1, we subtracted a number slightly smaller than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\min(P)</annotation></semantics></math>
from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
prior to raising the variable to the 4th power to induce substantial
nonlinearity into the fitted partial regression curve. The resulting
component-plus-residual plot for the transformed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
clearly reflects the resulting lack of fit, while the plot for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
is still reasonably linear.</p>
<p>Predictor effect plots with partial residuals show a different view
of the same situation by placing <code>P</code> rather than the
transformed <code>P</code> on the horizontal axis, and revealing that
the fitted nonlinear partial regression function fails to capture the
linear pattern of the data:</p>
<div class="sourceCode" id="cb39"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/effects/man/predictorEffects.html" class="external-link">predictorEffects</a></span><span class="op">(</span><span class="va">deq2</span>, residuals<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>, </span>
<span>     partial.residuals<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>span<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-21-1.png" width="768"></p>
<p>Recall that the blue lines represent the fitted model and the magenta
lines are for the smoothed partial residuals; discrepancy between the
two lines is indicative of lack of fit.</p>
</div>
</div>
<div class="section level2">
<h2 id="nonconstant-error-variance">Nonconstant Error Variance<a class="anchor" aria-label="anchor" href="#nonconstant-error-variance"></a>
</h2>
<p>Standard least-squares nonconstant variance (“heteroscedasticity”)
diagnostics extend straightforwardly to 2SLS regression. We can, for
example, plot studentized residuals versus fitted values to discern a
tendency for the variability of the former to change (typically to
increase) with the level of the latter. For the demand equation in
Kmenta’s model,</p>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">rstudent</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span>h<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-22-1.png" width="384"></p>
<p>which seems unproblematic.</p>
<p>A variation of this graph, suggested by <span class="citation">Fox
(2016)</span>, adapts Tukey’s <em>spread-level plot</em> <span class="citation">(Tukey 1977)</span> to graph the log of the absolute
studentized residuals versus the log of the fitted values, assuming that
the latter are positive. If a line fit to the plot has slope
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>,
then a variance-stabilizing power transformation is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mi>λ</mi></msup><mo>=</mo><msup><mi>y</mi><mrow><mn>1</mn><mo>−</mo><mi>b</mi></mrow></msup></mrow><annotation encoding="application/x-tex">y^\lambda = y^{1 - b}</annotation></semantics></math>.
Thus if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">b &gt; 0</annotation></semantics></math>,
the suggested transformation is <em>down</em> Tukey’s ladder of powers
and roots, with, for example,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>b</mi><mo>=</mo><mn>1</mn><mi>/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">\lambda = 1 - b = 1/2</annotation></semantics></math>
representing the square-root transformation,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mn>1</mn><mo>−</mo><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\lambda = 1 - b = 0</annotation></semantics></math>
the log transformation, and so on. For Kmenta’s model, we have</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/spreadLevelPlot.html" class="external-link">spreadLevelPlot</a></span><span class="op">(</span><span class="va">deq</span>, smooth<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>span<span class="op">=</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-23-1.png" width="384"></p>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Suggested power transformation:  -2.44685</span></span></code></pre>
<p>which suggests a slight tendency of spread to increase with level.
The transformation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mo>−</mo><mn>2.45</mn></mrow><annotation encoding="application/x-tex">\lambda = - 2.45</annotation></semantics></math>
seems strong, until we notice that the values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
are far from 0, and that the ratio of the largest to smallest values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow></msub><mi>/</mi><msub><mi>Q</mi><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi></mrow></msub><mo>=</mo><mn>106.23</mn><mi>/</mi><mn>92.42</mn><mo>=</mo><mn>1.15</mn></mrow><annotation encoding="application/x-tex">Q_{\mathrm{max}}/Q_{\mathrm{min}} = 106.23/92.42 = 1.15</annotation></semantics></math>
is close to 1, so that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Q</mi><mrow><mo>−</mo><mn>2.45</mn></mrow></msup><annotation encoding="application/x-tex">Q^{-2.45}</annotation></semantics></math>
is nearly a linear transformation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>—that
is, effectively no transformation at all:</p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Kmenta</span>, <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">Q</span>, <span class="va">Q</span><span class="op">^</span><span class="op">-</span><span class="fl">2.5</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/abline.html" class="external-link">abline</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">Q</span><span class="op">^</span><span class="op">-</span><span class="fl">2.5</span> <span class="op">~</span> <span class="va">Q</span>, data<span class="op">=</span><span class="va">Kmenta</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-24-1.png" width="384"></p>
<p>A common score test for nonconstant error variance in least-squares
regression, suggested by <span class="citation">Breusch and Pagan
(1979)</span>, is based on the model
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ε</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>γ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>γ</mi><mn>1</mn></msub><msub><mi>z</mi><mn>1</mn></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><msub><mi>γ</mi><mi>s</mi></msub><msub><mi>z</mi><mi>s</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
V(\varepsilon) = g(\gamma_0 + \gamma_1 z_1 + \cdots + \gamma_s z_s) 
</annotation></semantics></math> where the function
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">g()</annotation></semantics></math>
is unspecified and the variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><mi>…</mi><mo>,</mo><msub><mi>z</mi><mi>s</mi></msub></mrow><annotation encoding="application/x-tex">z_1, \ldots, z_s</annotation></semantics></math>
are predictors of the error variance. In the most common application,
independently proposed by <span class="citation">Cook and Weisberg
(1983)</span>, there is one
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>,
the fitted values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>y</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{y}</annotation></semantics></math>
from the regression, although it is also common to use the regressors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
from the primary regression as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>s.
The test is implemented by regressing the squared standardized residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup><mi>/</mi><msup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">e_i^2/\widehat{\sigma}^2</annotation></semantics></math>
on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>z</mi><annotation encoding="application/x-tex">z</annotation></semantics></math>s,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mn>2</mn></msup><mo>=</mo><mo>∑</mo><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup><mi>/</mi><mi>n</mi></mrow><annotation encoding="application/x-tex">\widehat{\sigma}^2 = \sum e_i^2/n</annotation></semantics></math>.
The regression sum of squares for this auxiliary regression divided by 2
is then asymptotically distributed as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>χ</mi><mi>s</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\chi^2_s</annotation></semantics></math>
under the null hypothesis of constant error variance.</p>
<p>The Breusch-Pagan/Cook-Weisberg test is easily adaptable to 2SLS
regression, as implemented by the <code><a href="https://rdrr.io/pkg/car/man/ncvTest.html" class="external-link">ncvTest()</a></code> function in the
<strong>car</strong> package. For Kmenta’s demand equation:</p>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html" class="external-link">ncvTest</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Non-constant Variance Score Test </span></span>
<span><span class="co">## Variance formula: ~ fitted.values </span></span>
<span><span class="co">## Chisquare = 0.2390325, Df = 1, p = 0.62491</span></span></code></pre>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html" class="external-link">ncvTest</a></span><span class="op">(</span><span class="va">deq</span>, var <span class="op">=</span> <span class="op">~</span> <span class="va">P</span> <span class="op">+</span> <span class="va">D</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Non-constant Variance Score Test </span></span>
<span><span class="co">## Variance formula: ~ P + D </span></span>
<span><span class="co">## Chisquare = 0.2392964, Df = 2, p = 0.88723</span></span></code></pre>
<p>Here, the first test is against the fitted values and the second more
general test is against the explanatory variables in the demand
equation; the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
for both tests are large, suggesting little evidence against the
hypothesis of constant variance.</p>
<p>Remedies for nonconstant variance in 2SLS regression are similar to
those in least-squares regression:</p>
<ul>
<li><p>We’ve already suggested that if the error variance increases (or
decreases) with the level of the response, and if the response is
positive, then we might be able to stabilize the error variance by
power-transforming the response.</p></li>
<li><p>If, alternatively, we know the variance of the errors up to a
constant of proportionality, then we can use inverse-variance weights
for the 2SLS estimator. The <code><a href="../reference/ivreg.html">ivreg()</a></code> function supports
weighted 2SLS regression, and the diagnostics in the
<strong>ivreg</strong> package work with weighted 2SLS fits (see the
next section).</p></li>
<li><p>Finally, we can employ a <em>“sandwich” estimator</em> of the
coefficient covariance matrix in 2SLS <span class="citation">(or the
<em>bootstrap</em>: see, e.g., Davison and Hinkley 1997)</span> to
correct standard errors for nonconstant error variance, much as in
least-squares regression as proposed by <span class="citation">P. J.
Huber (1967)</span> and <span class="citation">White (1980; also see
Long and Ervin 2000)</span>.</p></li>
</ul>
<p>The <strong>ivreg</strong> package supports the
<code><a href="https://sandwich.R-Forge.R-project.org/reference/sandwich.html" class="external-link">sandwich()</a></code> function in the <strong>sandwich</strong>
package <span class="citation">(Zeileis 2006)</span>. For the Kmenta
example, where evidence of nonconstant error variance is slight, the
sandwich standard errors are similar to, indeed slightly smaller than,
the conventional 2SLS standard errors:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">deq</span>, vcov<span class="op">=</span><span class="fu">sandwich</span><span class="fu">::</span><span class="va"><a href="https://sandwich.R-Forge.R-project.org/reference/sandwich.html" class="external-link">sandwich</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">## -3.4305 -1.2432 -0.1895  1.5762  2.4920 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 94.63330    5.14745  18.384 1.18e-12 ***</span></span>
<span><span class="co">## P           -0.24356    0.07590  -3.209  0.00515 ** </span></span>
<span><span class="co">## D            0.31399    0.04293   7.315 1.21e-06 ***</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   2  16   142.340 6.43e-11 ***</span></span>
<span><span class="co">## Wu-Hausman         1  16    21.898 0.000251 ***</span></span>
<span><span class="co">## Sargan             1  NA     2.983 0.084137 .  </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 1.966 on 17 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.7548,  Adjusted R-squared: 0.726 </span></span>
<span><span class="co">## Wald test: 34.41 on 2 and 17 DF,  p-value: 1.055e-06</span></span></code></pre>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">SEs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/sandwich.html" class="external-link">sandwich</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>             <span class="fl">4</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">SEs</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"sandwich"</span>, <span class="st">"conventional"</span><span class="op">)</span></span>
<span><span class="va">SEs</span></span></code></pre></div>
<pre><code><span><span class="co">##             sandwich conventional</span></span>
<span><span class="co">## (Intercept)   5.1475       7.9208</span></span>
<span><span class="co">## P             0.0759       0.0965</span></span>
<span><span class="co">## D             0.0429       0.0469</span></span></code></pre>
<p>We’ll modify Kmenta’s data to reflect nonconstant error variance,
regenerating the data as Kmenta did originally from the reduced-form
equations, expressing the endogenous variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
as functions of the exogenous variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>,
and reduced-form errors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ν</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\nu_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ν</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\nu_2</annotation></semantics></math>:</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">Kmenta2</span> <span class="op">&lt;-</span> <span class="va">Kmenta</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"D"</span>, <span class="st">"F"</span>, <span class="st">"A"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">492365</span><span class="op">)</span> <span class="co"># for reproducibility</span></span>
<span><span class="va">Kmenta2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">within</a></span><span class="op">(</span><span class="va">Kmenta2</span>, <span class="op">{</span></span>
<span>    <span class="va">EQ</span> <span class="op">&lt;-</span> <span class="fl">75.25</span> <span class="op">+</span> <span class="fl">0.1125</span><span class="op">*</span><span class="va">D</span> <span class="op">+</span> <span class="fl">0.1250</span><span class="op">*</span><span class="cn">F</span> <span class="op">+</span> <span class="fl">0.225</span><span class="op">*</span><span class="va">A</span></span>
<span>    <span class="va">EP</span> <span class="op">&lt;-</span> <span class="fl">85.00</span> <span class="op">+</span> <span class="fl">0.7500</span><span class="op">*</span><span class="va">D</span> <span class="op">-</span> <span class="fl">0.5000</span><span class="op">*</span><span class="cn">F</span> <span class="op">-</span> <span class="fl">0.900</span><span class="op">*</span><span class="va">A</span></span>
<span>    <span class="va">d1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span>    <span class="va">d2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">20</span><span class="op">)</span></span>
<span>    <span class="va">v1</span> <span class="op">&lt;-</span> <span class="fl">2</span><span class="op">*</span><span class="va">d1</span></span>
<span>    <span class="va">v2</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">0.5</span><span class="op">*</span><span class="va">v1</span> <span class="op">+</span> <span class="va">d2</span></span>
<span>    <span class="va">w</span> <span class="op">&lt;-</span> <span class="fl">3</span><span class="op">*</span><span class="op">(</span><span class="va">EQ</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">EQ</span><span class="op">)</span> <span class="op">+</span> <span class="fl">0.1</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">EQ</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">EQ</span><span class="op">)</span><span class="op">)</span> </span>
<span>    <span class="va">v1</span> <span class="op">&lt;-</span> <span class="va">v1</span><span class="op">*</span><span class="va">w</span> <span class="co"># inducing nonconstant variance</span></span>
<span>    <span class="va">Q</span> <span class="op">&lt;-</span> <span class="va">EQ</span> <span class="op">+</span> <span class="va">v1</span></span>
<span>    <span class="va">P</span> <span class="op">&lt;-</span> <span class="va">EP</span> <span class="op">+</span> <span class="va">v2</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code></pre></div>
<p>Plotting the sampled reduced-form errors <code>v1</code> against the
expectation of <code>Q</code> shows a clear heteroscedastic pattern:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Kmenta2</span>, <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">EQ</span>, <span class="va">v1</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-28-1.png" width="384"></p>
<p>Then refitting the demand equation to the new data set, we get</p>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deq2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">deq</span>, data<span class="op">=</span><span class="va">Kmenta2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">deq2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta2)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">## -8.7805 -1.4920  0.1067  1.9745  5.0629 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 111.42774   11.36644   9.803 2.07e-08 ***</span></span>
<span><span class="co">## P            -0.39072    0.13979  -2.795  0.01243 *  </span></span>
<span><span class="co">## D             0.28415    0.07965   3.567  0.00237 ** </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   2  16   122.011 2.06e-10 ***</span></span>
<span><span class="co">## Wu-Hausman         1  16    22.883 0.000203 ***</span></span>
<span><span class="co">## Sargan             1  NA     0.221 0.638221    </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 3.283 on 17 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.521,   Adjusted R-squared: 0.4646 </span></span>
<span><span class="co">## Wald test: 6.695 on 2 and 17 DF,  p-value: 0.007172</span></span></code></pre>
<p>and the nonconstant error variance is clearly reflected in
diagnostics; for example,</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/spreadLevelPlot.html" class="external-link">spreadLevelPlot</a></span><span class="op">(</span><span class="va">deq2</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-30-1.png" width="384"></p>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Suggested power transformation:  -22.57328</span></span></code></pre>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html" class="external-link">ncvTest</a></span><span class="op">(</span><span class="va">deq2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Non-constant Variance Score Test </span></span>
<span><span class="co">## Variance formula: ~ fitted.values </span></span>
<span><span class="co">## Chisquare = 6.690435, Df = 1, p = 0.0096932</span></span></code></pre>
<p>The extreme value of the suggested power transformation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>
from the spread-level plot,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mo>=</mo><mo>−</mo><mn>23</mn></mrow><annotation encoding="application/x-tex">\lambda =-23</annotation></semantics></math>,
reflects (as we noted previously) the fact that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>max</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>Q</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\max(Q)/\min(Q)</annotation></semantics></math>
isn’t much larger than 1.</p>
<p>In our example, the sandwich standard errors are not very different
from the conventional standard errors:</p>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">SEs2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">cbind</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu">sandwich</span><span class="fu">::</span><span class="fu"><a href="https://sandwich.R-Forge.R-project.org/reference/sandwich.html" class="external-link">sandwich</a></span><span class="op">(</span><span class="va">deq2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html" class="external-link">diag</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">deq2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>, </span>
<span>             <span class="fl">4</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">SEs2</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"sandwich"</span>, <span class="st">"conventional"</span><span class="op">)</span></span>
<span><span class="va">SEs2</span></span></code></pre></div>
<pre><code><span><span class="co">##             sandwich conventional</span></span>
<span><span class="co">## (Intercept)  13.7782      11.3664</span></span>
<span><span class="co">## P             0.1702       0.1398</span></span>
<span><span class="co">## D             0.0848       0.0797</span></span></code></pre>
<p>As mentioned, bootstrapping provides an alternative to sandwich
standard errors as a correction for nonconstant error variance, and the
<strong>ivreg</strong> package supplies an <code>"ivreg"</code> method
for the <code><a href="https://rdrr.io/pkg/car/man/Boot.html" class="external-link">Boot()</a></code> function in the <strong>car</strong>
package, implementing the case-resampling bootstrap, and returning an
object of class <code>"boot"</code> suitable for use with functions in
the <strong>boot</strong> package <span class="citation">(Davison and
Hinkley 1997; Canty and Ripley 2020)</span>. By default,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>=</mo><mn>999</mn></mrow><annotation encoding="application/x-tex">R = 999</annotation></semantics></math>
bootstrap replications are generated. For example:</p>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">set.seed</span> <span class="op">&lt;-</span> <span class="fl">869255</span> <span class="co"># for reproducibility</span></span>
<span><span class="va">b.deq2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/Boot.html" class="external-link">Boot</a></span><span class="op">(</span><span class="va">deq2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">deq2</span>, vcov.<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html" class="external-link">vcov</a></span><span class="op">(</span><span class="va">b.deq2</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta2)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">## -8.7805 -1.4920  0.1067  1.9745  5.0629 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 111.42774   16.70085   6.672 3.93e-06 ***</span></span>
<span><span class="co">## P            -0.39072    0.19678  -1.986  0.06345 .  </span></span>
<span><span class="co">## D             0.28415    0.09762   2.911  0.00974 ** </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   2  16   122.011 2.06e-10 ***</span></span>
<span><span class="co">## Wu-Hausman         1  16    22.883 0.000203 ***</span></span>
<span><span class="co">## Sargan             1  NA     0.221 0.638221    </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 3.283 on 17 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.521,   Adjusted R-squared: 0.4646 </span></span>
<span><span class="co">## Wald test:  4.32 on 2 and 17 DF,  p-value: 0.03042</span></span></code></pre>
<p>The bootstrap standard errors are larger than the conventional or
sandwich standard errors for this example.</p>
<p>Bootstrap confidence intervals can also be computed from the object
returned by <code><a href="https://rdrr.io/pkg/car/man/Boot.html" class="external-link">Boot()</a></code>, by default reporting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><msub><mi>C</mi><mi>a</mi></msub></mrow><annotation encoding="application/x-tex">BC_a</annotation></semantics></math>
(<em>bias-corrected, accelerated</em>) intervals (see the documentation
for <code>confint.boot()</code> in the <strong>car</strong>
package):</p>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/confint.html" class="external-link">confint</a></span><span class="op">(</span><span class="va">b.deq2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Bootstrap bca confidence intervals</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##                   2.5 %      97.5 %</span></span>
<span><span class="co">## (Intercept) 64.38447188 137.8541642</span></span>
<span><span class="co">## P           -0.61242364   0.2463746</span></span>
<span><span class="co">## D            0.05739467   0.4253580</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="weighted-2sls-regression">Weighted 2SLS Regression<a class="anchor" aria-label="anchor" href="#weighted-2sls-regression"></a>
</h2>
<p>Suppose that we modify the regression model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>X</mi><mi>β</mi><mo>+</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">y = X \beta + \varepsilon</annotation></semantics></math>
so that now
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi><mo>∼</mo><msub><mi>N</mi><mi>n</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><msup><mi>W</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\varepsilon \sim N_n(0, \sigma^2 W^{-1})</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>w</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">W = \mathrm{diag}\{w_i\}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math>
diagonal matrix of known inverse-variance weights; that is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>ε</mi><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msup><mi>σ</mi><mn>2</mn></msup><mi>/</mi><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">V(\varepsilon_i) = \sigma^2/w_i</annotation></semantics></math>.
As before, some of the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
may be correlated with the errors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ε</mi><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math>,
but we have sufficient instrumental variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>
that are independent of the errors.</p>
<p>Then the <em>weighted 2SLS</em> estimator is
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mrow><mi mathvariant="normal">W</mi><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>X</mi><mi>⊤</mi></msup><mi>W</mi><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>W</mi><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>W</mi><mi>X</mi><mo stretchy="true" form="postfix">]</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>X</mi><mi>⊤</mi></msup><mi>W</mi><mi>Z</mi><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>Z</mi><mi>⊤</mi></msup><mi>W</mi><mi>Z</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><msup><mi>Z</mi><mi>⊤</mi></msup><mi>W</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">
b_{\mathrm{W2SLS}} = [X^\top W Z(Z^\top W Z)^{-1} Z^\top W X]^{-1} X^\top W Z (Z^\top W Z)^{-1} Z^\top W y
</annotation></semantics></math> Alternatively, we can treat the two
stages of 2SLS as <em>weighted least squares (WLS)</em> problems, in
each stage minimizing the weighted sum of squared residuals. The
<code><a href="../reference/ivreg.html">ivreg()</a></code> function computes the weighted 2SLS estimator in
this manner.</p>
<p>Phillips’s updating formulas for 2SLS regression could also be
modified for the weighted case, but a simpler approach (which is evident
in the formula for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mrow><mi mathvariant="normal">W</mi><mn mathvariant="normal">2</mn><mi mathvariant="normal">S</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">S</mi></mrow></msub><annotation encoding="application/x-tex">b_{\mathrm{W2SLS}}</annotation></semantics></math>
above) is to convert the weighted 2SLS problem into an unweighted
problem, by transforming the data to constant variance using
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mo>=</mo><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">g</mi></mrow><mo stretchy="false" form="prefix">{</mo><msqrt><msub><mi>w</mi><mi>i</mi></msub></msqrt><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">W^{1/2} = \mathrm{diag}\{\sqrt{w_i}\}</annotation></semantics></math>,
the Cholesky square root of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>.
The square root of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
is particularly simple because
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
is diagonal. Then in Phillips’s updating formulas, we replace
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mo>*</mo></msup><mo>=</mo><msup><mi>W</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>y</mi></mrow><annotation encoding="application/x-tex">y^* = W^{1/2}y</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mo>*</mo></msup><mo>=</mo><msup><mi>W</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>X</mi></mrow><annotation encoding="application/x-tex">X^* = W^{1/2}X</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Z</mi><mo>*</mo></msup><mo>=</mo><msup><mi>W</mi><mrow><mn>1</mn><mi>/</mi><mn>2</mn></mrow></msup><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z^* = W^{1/2}Z</annotation></semantics></math>.</p>
<p>For the modified <code>Kmenta2</code> data, we know that the
variances of the errors in the demand equation are inversely
proportional to the variable <code>w</code>. This is of course
artificial knowledge, reflecting the manner in which the data were
constructed. The weighted 2SLS estimator is therefore computed as</p>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deqw</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">deq</span>, data<span class="op">=</span><span class="va">Kmenta2</span>, weights<span class="op">=</span><span class="fl">1</span><span class="op">/</span><span class="va">w</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">deqw</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta2, weights = 1/w)</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##      Min       1Q   Median       3Q      Max </span></span>
<span><span class="co">## -5.43959 -1.66625 -0.08906  1.81440  3.41694 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##              Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 107.88374   10.23415  10.542 7.11e-09 ***</span></span>
<span><span class="co">## P            -0.33586    0.12240  -2.744   0.0138 *  </span></span>
<span><span class="co">## D             0.26347    0.04405   5.981 1.49e-05 ***</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   2  16   101.172 8.31e-10 ***</span></span>
<span><span class="co">## Wu-Hausman         1  16    20.105 0.000376 ***</span></span>
<span><span class="co">## Sargan             1  NA     0.087 0.767864    </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 2.308 on 17 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.7166,  Adjusted R-squared: 0.6833 </span></span>
<span><span class="co">## Wald test: 18.79 on 2 and 17 DF,  p-value: 4.95e-05</span></span></code></pre>
<p>Plotting studentized residuals against fitted values and testing for
nonconstant error variance don’t indicate a heteroscedasticity problem,
but there is a relatively large studentized residual of about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">-3</annotation></semantics></math>
that stands out somewhat from the other values:</p>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html" class="external-link">ncvTest</a></span><span class="op">(</span><span class="va">deqw</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Non-constant Variance Score Test </span></span>
<span><span class="co">## Variance formula: ~ fitted.values </span></span>
<span><span class="co">## Chisquare = 4.21029, Df = 1, p = 0.040179</span></span></code></pre>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">deqw</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">rstudent</a></span><span class="op">(</span><span class="va">deqw</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-36-1.png" width="384"></p>
<p>A Bonferroni outlier test suggests that the studentized residual
isn’t unusually large, and once more we’re in the unusual situation of
knowing that the model is correct.</p>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/outlierTest.html" class="external-link">outlierTest</a></span><span class="op">(</span><span class="va">deqw</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## No Studentized residuals with Bonferroni p &lt; 0.05</span></span>
<span><span class="co">## Largest |rstudent|:</span></span>
<span><span class="co">##       rstudent unadjusted p-value Bonferroni p</span></span>
<span><span class="co">## 1937 -3.135343          0.0063887      0.12777</span></span></code></pre>
</div>
<div class="section level2">
<h2 id="collinearity-diagnostics">Collinearity Diagnostics<a class="anchor" aria-label="anchor" href="#collinearity-diagnostics"></a>
</h2>
<p>In addition to unusual-data diagnostics, <span class="citation">Belsley, Kuh, and Welsch (1980)</span> briefly extend
their approach to collinearity diagnostics to 2SLS regression. We
believe that this approach, which assimilates collinearity to numerical
instability, is flawed, in that it takes into account “collinearity with
the intercept.” That is, regressors with values far from 0 have large
sums of products with the constant regressor, producing a large standard
error of the intercept, and simply reflecting the fact that the
intercept extrapolates the fitted regression surface far beyond the
range of the data.</p>
<p><span class="citation">Fox and Monette (1992)</span> describe an
alternative approach to collinearity diagnostics in linear models fit by
least squares based on <em>generalized variance-inflation factors</em>.
The implementation of generalized variance inflation factors in the
<code><a href="https://rdrr.io/pkg/car/man/vif.html" class="external-link">vif()</a></code> function in the <strong>car</strong> package, which
employs the estimated covariance matrix of the coefficients, applies in
general to models with linear predictors, including linear models
estimated by 2SLS.</p>
<p>For example, for the demand equation in Kmenta’s model:</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html" class="external-link">vif</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##        P        D </span></span>
<span><span class="co">## 1.231124 1.231124</span></span></code></pre>
<p>Taking the square-roots of the VIFs puts them on the coefficient
standard-error scale. That is, the standard errors of the coefficients
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
are 23% larger than they would be if the estimated coefficients were
uncorrelated (which is equivalent to the columns of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>X</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{X}</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
in the second-stage regression being uncorrelated). When, as here, each
term in the model has just one coefficient, generalized and ordinary
variance-inflation factors coincide. The equality of the VIFs for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
is peculiar to the case of two regressors (beyond the regression
constant).</p>
<p><em>Marginal/conditional</em> plots, produced by the
<code><a href="https://rdrr.io/pkg/car/man/mcPlots.html" class="external-link">mcPlots()</a></code> function in the <strong>car</strong> package,
superimpose the added-variable plots on corresponding marginal
scatterplots for the regressors. These graphs allow us to visualize the
reduction in precision of estimation of each coefficient due to
collinearity, which reduces the conditional variation of a regressor
relative to its marginal variation. for example, for the demand
equation:</p>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/mcPlots.html" class="external-link">mcPlots</a></span><span class="op">(</span><span class="va">deq</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-39-1.png" width="768"></p>
<p>The blue points in each panel represent the marginal scatterplot and
the magenta points represent the (partial) added-variable plot, with the
arrows showing the relationship between the two sets of points.</p>
</div>
<div class="section level2">
<h2 id="robust-two-stage-instrumental-variables-regression">Robust Two-Stage Instrumental-Variables Regression<a class="anchor" aria-label="anchor" href="#robust-two-stage-instrumental-variables-regression"></a>
</h2>
<p>The <code><a href="../reference/ivreg.html">ivreg()</a></code> function also implements robust two-stage
regression with M and MM estimators <span class="citation">(P. Huber and
Ronchetti 2009)</span> for both stages, via the <code>rlm()</code>
function in the <strong>MASS</strong> package <span class="citation">(Venables and Ripley 2002)</span>, by specifying,
respectively, the argument <code>type = "M"</code> or
<code>type = "MM"</code>. The M or MM estimator is applied separately to
each first-stage regression and to the second-stage regression, thus
down-weighting outliers in all of the regressions. The default for the
<code>type</code> argument to <code><a href="../reference/ivreg.html">ivreg()</a></code>,
<code>type = "OLS"</code>, produces the traditional 2SLS estimator.</p>
<p>The details of implementation are largely straightforward. Applying
the M or MM estimator to the first stage requires a separate regression
for each endogenous regressor, rather than a single multivariate
regression, because the robustness weights are generally different for
each of the first-stage regressions.</p>
<p>As well, rather than basing the estimated error standard deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>σ</mi><mo accent="true">̂</mo></mover><annotation encoding="application/x-tex">\widehat{\sigma}</annotation></semantics></math>
on the sum of squared model residuals, we use a robust estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
based on the median absolute residual—the median absolute deviation (or
MAD) of the residuals around a “center” of 0. The MAD of the residuals
is multiplied by 1.4826 to provide a properly scaled estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
when the error distribution is normal: That is, the MAD estimates the
0.75 quantile of the error distribution, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi>/</mi><msup><mi>Φ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.75</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1.4825</mn></mrow><annotation encoding="application/x-tex">1/\Phi^{-1}(0.75) = 1.4825</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Φ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mrow><mo stretchy="true" form="prefix">(</mo><mo>⋅</mo><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Phi^{-1}(\cdot)</annotation></semantics></math>
is the quantile function of the standard-normal distribution.</p>
<p>To illustrate, we apply a two-stage MM regression to the estimation
of the demand equation in the corrupted Kmenta data, and compare the
result to the traditional 2SLS estimator applied to the uncorrupted
data, to the 2SLS estimator applied to the corrupted data, and to the
2SLS estimator deleting the bad case:</p>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">deq.mm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html" class="external-link">update</a></span><span class="op">(</span><span class="va">deq1</span>, method <span class="op">=</span> <span class="st">"MM"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">deq.mm</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Call:</span></span>
<span><span class="co">## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1, method = "MM")</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residuals:</span></span>
<span><span class="co">##       Min        1Q    Median        3Q       Max </span></span>
<span><span class="co">## -13.22380  -1.56688   0.04599   1.12301   2.55786 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Coefficients:</span></span>
<span><span class="co">##             Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">## (Intercept) 91.09249   10.62357   8.575 1.40e-07 ***</span></span>
<span><span class="co">## P           -0.23742    0.11353  -2.091   0.0518 .  </span></span>
<span><span class="co">## D            0.34678    0.05688   6.097 1.19e-05 ***</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Diagnostic tests:</span></span>
<span><span class="co">##                  df1 df2 statistic  p-value    </span></span>
<span><span class="co">## Weak instruments   2  16    88.025 2.32e-09 ***</span></span>
<span><span class="co">## Wu-Hausman         1  16     0.803  0.38339    </span></span>
<span><span class="co">## Sargan             1  NA    10.330  0.00131 ** </span></span>
<span><span class="co">## ---</span></span>
<span><span class="co">## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Residual standard error: 2.08 on 17 degrees of freedom</span></span>
<span><span class="co">## Multiple R-Squared: 0.09864, Adjusted R-squared: -0.007398 </span></span>
<span><span class="co">## Wald test: 18.59 on 2 and 17 DF,  p-value: 5.262e-05</span></span></code></pre>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/compareCoefs.html" class="external-link">compareCoefs</a></span><span class="op">(</span><span class="va">deq</span>, <span class="va">deq1</span>, <span class="va">deq1.20</span>, <span class="va">deq.mm</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Calls:</span></span>
<span><span class="co">## 1: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)</span></span>
<span><span class="co">## 2: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1)</span></span>
<span><span class="co">## 3: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1, subset = -20)</span></span>
<span><span class="co">## 4: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1, method = "MM")</span></span>
<span><span class="co">## </span></span>
<span><span class="co">##             Model 1 Model 2 Model 3 Model 4</span></span>
<span><span class="co">## (Intercept)   94.63  117.96   92.42   91.09</span></span>
<span><span class="co">## SE             7.92   11.64    9.67   10.62</span></span>
<span><span class="co">##                                            </span></span>
<span><span class="co">## P           -0.2436 -0.4054 -0.2300 -0.2374</span></span>
<span><span class="co">## SE           0.0965  0.1417  0.1047  0.1135</span></span>
<span><span class="co">##                                            </span></span>
<span><span class="co">## D            0.3140  0.2351  0.3233  0.3468</span></span>
<span><span class="co">## SE           0.0469  0.0690  0.0527  0.0569</span></span>
<span><span class="co">## </span></span></code></pre>
<p>The estimates produced by the robust two-stage estimator are very
similar to those for the 2SLS estimator applied to the uncorrupted data
and when the bad case is deleted from the corrupted data.</p>
<p>It’s instructive to look at the robustness weights for the first- and
second-stage MM estimators:</p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/weights.html" class="external-link">weights</a></span><span class="op">(</span><span class="va">deq.mm</span>, type <span class="op">=</span> <span class="st">"robustness"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##              P   stage_2</span></span>
<span><span class="co">## 1922 0.9744449 0.9843442</span></span>
<span><span class="co">## 1923 0.9650580 0.9813862</span></span>
<span><span class="co">## 1924 0.9952293 0.8700471</span></span>
<span><span class="co">## 1925 0.9998998 0.9618035</span></span>
<span><span class="co">## 1926 0.9835828 0.8952875</span></span>
<span><span class="co">## 1927 0.9996116 0.9752454</span></span>
<span><span class="co">## 1928 0.9668182 0.9525911</span></span>
<span><span class="co">## 1929 0.6417885 0.5318460</span></span>
<span><span class="co">## 1930 0.8028481 0.9085481</span></span>
<span><span class="co">## 1931 0.8905474 0.7734043</span></span>
<span><span class="co">## 1932 0.9775792 0.9982440</span></span>
<span><span class="co">## 1933 0.9985961 0.9050868</span></span>
<span><span class="co">## 1934 0.9691516 0.9241239</span></span>
<span><span class="co">## 1935 0.8868046 0.9999572</span></span>
<span><span class="co">## 1936 0.7238126 0.8787684</span></span>
<span><span class="co">## 1937 0.8387680 0.5293154</span></span>
<span><span class="co">## 1938 0.9414371 0.9986454</span></span>
<span><span class="co">## 1939 0.5325076 0.6927505</span></span>
<span><span class="co">## 1940 0.9996695 0.9783155</span></span>
<span><span class="co">## 1941 0.9811313 0.0000000</span></span></code></pre>
<p>Thus, 1941 is so discrepant in the second stage that it receives 0
weight in the robust fit.</p>
<p>Most of the diagnostic and other methods for <code>"ivreg"</code>
objects also work with robust two-stage regression models. To compute
deletion statistics like studentized residuals, dfbeta, and Cook’s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>,
we use the robustness weights for the M or MM second-stage regression
(multiplied by prespecified variance weights, if these are employed) as
if they were weights in a 2SLS fit. This is, of course, an
approximation: To get exact deletion statistics would require literally
omitting each case in turn and recalculating the first- and second-stage
robust regressions, an unattractive prospect, because the robust
regressions are iterative.</p>
<p>Here, for example, is a graph of second-stage hatvalues, studentized
residuals, and Cook’s distances for the 2SMM estimator applied to the
corrupted Kmenta data:</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/car/man/influencePlot.html" class="external-link">influencePlot</a></span><span class="op">(</span><span class="va">deq.mm</span><span class="op">)</span></span></code></pre></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-42-1.png" width="384"></p>
<pre><code><span><span class="co">##      StudRes        Hat      CookD</span></span>
<span><span class="co">## 1  1.6778245 0.13345298 0.07043221</span></span>
<span><span class="co">## 2  0.2892163 0.35576176 0.01023249</span></span>
<span><span class="co">## 3 -1.3354194 0.28104141 0.13365515</span></span>
<span><span class="co">## 4 -2.7083737 0.09236777 0.06489585</span></span>
<span><span class="co">## 5 -0.3801649 0.47603246 0.03387451</span></span>
<span><span class="co">## 6 -7.6841103 0.00000000 0.00000000</span></span></code></pre>
<p>Because the hatvalue for 1941 in the robust second-stage fit is 0, so
is the Cook’s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
for this case, and the “bubble” for 1941 has 0 area (and is therefore
invisible). The bad case, however, have a very large (negative)
studentized residual of nearly
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">-8</annotation></semantics></math>.</p>
</div>
<div class="section level2">
<h2 id="concluding-remarks">Concluding Remarks<a class="anchor" aria-label="anchor" href="#concluding-remarks"></a>
</h2>
<p>Careful regression analysis requires methods for looking effectively
at the data. Many potential problems can be addressed by examining the
data <em>prior</em> to fitting a regression model, decreasing (if not
eliminating) the necessity for <em>post-fit</em> diagnostics. No doubt
careful data analysts employing 2SLS have always done this.
Nevertheless, having methods that allow one to subject a regression
model fit by 2SLS to criticism will in at least some cases suggest
improvements to the model or perhaps corrections to the data.</p>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Basmann1957" class="csl-entry">
Basmann, R. L. 1957. <span>“A Generalized Classical Method of Linear
Estimation of Coefficients in a Structural Equation.”</span>
<em>Econometrica</em> 25: 77–83. <a href="https://doi.org/10.2307/1907743" class="external-link">https://doi.org/10.2307/1907743</a>.
</div>
<div id="ref-BelsleyKuhWelsch1980" class="csl-entry">
Belsley, D. A., E. Kuh, and R. E. Welsch. 1980. <em>Regression
Diagnostics: Identifying Influential Data and Sources of
Collinearity</em>. New York: John Wiley &amp; Sons. <a href="https://doi.org/10.1002/0471725153" class="external-link">https://doi.org/10.1002/0471725153</a>.
</div>
<div id="ref-BreuschPagan1979" class="csl-entry">
Breusch, T. S., and A. R. Pagan. 1979. <span>“A Simple Test for
Heteroscedasticity and Random Coefficient Variation.”</span>
<em>Econometrica</em> 47: 1287–94. <a href="https://doi.org/10.2307/1911963" class="external-link">https://doi.org/10.2307/1911963</a>.
</div>
<div id="ref-boot" class="csl-entry">
Canty, Angelo, and Brian D. Ripley. 2020. <em>: Bootstrap <span>R</span>
(<span>S-PLUS</span>) Functions</em>. <a href="https://CRAN.R-project.org/package=boot" class="external-link">https://CRAN.R-project.org/package=boot</a>.
</div>
<div id="ref-ClevelandGrosseShyu1992" class="csl-entry">
Cleveland, W. S., E. Grosse, and W. M. Shyu. 1992. <span>“Local
Regression Models.”</span> In <em>Statistical Models in
<span>S</span></em>, edited by J. M. Chambers and T. J. Hastie, 309–76.
Pacific Grove <span>CA</span>: Wadsworth &amp; Brooks/Cole.
</div>
<div id="ref-Cook1993" class="csl-entry">
Cook, R. D. 1993. <span>“Exploring Partial Residual Plots.”</span>
<em>Technometrics</em> 35: 351–62. <a href="https://doi.org/10.1080/00401706.1993.10485350" class="external-link">https://doi.org/10.1080/00401706.1993.10485350</a>.
</div>
<div id="ref-CookCroosDabrera1998" class="csl-entry">
Cook, R. D., and R. Croos-Dabrera. 1998. <span>“Partial Residual Plots
in Generalized Linear Models.”</span> <em>Journal of the American
Statistical Association</em> 93: 730–39. <a href="https://doi.org/10.1080/01621459.1998.10473725" class="external-link">https://doi.org/10.1080/01621459.1998.10473725</a>.
</div>
<div id="ref-CookWeisberg1983" class="csl-entry">
Cook, R. D., and S. Weisberg. 1983. <span>“Diagnostics for
Heteroscedasticity in Regression.”</span> <em>Biometrika</em> 70: 1–10.
<a href="https://doi.org/10.1093/biomet/70.1.1" class="external-link">https://doi.org/10.1093/biomet/70.1.1</a>.
</div>
<div id="ref-DavisonHinkley1997" class="csl-entry">
Davison, A. C., and D. V. Hinkley. 1997. <em>Bootstrap Methods and Their
Applications</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-Fox2016" class="csl-entry">
Fox, J. 2016. <em>Applied Regression Analysis and Generalized Linear
Models</em>. 3rd ed. Thousand Oaks: Sage.
</div>
<div id="ref-FoxMonette1992" class="csl-entry">
Fox, J., and G. Monette. 1992. <span>“Generalized Collinearity
Diagnostics.”</span> <em>Journal of the American Statistical
Association</em>, 178–83. <a href="https://doi.org/10.1080/01621459.1992.10475190" class="external-link">https://doi.org/10.1080/01621459.1992.10475190</a>.
</div>
<div id="ref-sem" class="csl-entry">
Fox, J., Z. Nie, and J. Byrnes. 2020. <em><span class="nocase">sem</span>: Structural Equation Models</em>. <a href="https://CRAN.R-project.org/package=sem" class="external-link">https://CRAN.R-project.org/package=sem</a>.
</div>
<div id="ref-FoxWeisberg2018" class="csl-entry">
Fox, J., and S. Weisberg. 2018. <span>“Visualizing Fit and Lack of Fit
in Complex Regression Models with Predictor Effect Plots and Partial
Residuals.”</span> <em>Journal of Statistical Software</em> 87 (9):
1–27. <a href="https://doi.org/10.18637/jss.v087.i09" class="external-link">https://doi.org/10.18637/jss.v087.i09</a>.
</div>
<div id="ref-FoxWeisberg2019" class="csl-entry">
———. 2019. <em>An <span>R</span> Companion to Applied Regression</em>.
3rd ed. Thousand Oaks: Sage.
</div>
<div id="ref-Greene2003" class="csl-entry">
Greene, W. H. 2003. <em>Econometric Analysis</em>. 5th ed. Upper Saddle
River: Prentice Hall.
</div>
<div id="ref-Huber1967" class="csl-entry">
Huber, P. J. 1967. <span>“The Behavior of Maximum Likelihood Estimation
Under Nonstandard Conditions.”</span> In <em>Proceedings of the Fifth
<span>B</span>erkeley Symposium on Mathematical Statistics and
Probability</em>, edited by L. M. LeCam and J. Neyman. Berkeley:
University of California Press.
</div>
<div id="ref-HuberRonchetti2009" class="csl-entry">
Huber, P., and E. M. Ronchetti. 2009. <em>Robust Statistics</em>. 2nd
ed. Hoboken <span>NJ</span>: Wiley.
</div>
<div id="ref-KleiberZeileis2008" class="csl-entry">
Kleiber, C., and A. Zeileis. 2008. <em>Applied Econometrics with
<span>R</span></em>. New York: Springer-Verlag.
</div>
<div id="ref-Kmenta1986" class="csl-entry">
Kmenta, J. 1986. <em>Elements of Econometrics</em>. 2nd ed. New York:
Macmillan.
</div>
<div id="ref-LongErvin2000" class="csl-entry">
Long, J. S., and L. H. Ervin. 2000. <span>“Using Heteroscedasticity
Consistent Standard Errors in the Linear Regression Model.”</span>
<em>The American Statistician</em> 54: 217–24. <a href="https://doi.org/10.1080/00031305.2000.10474549" class="external-link">https://doi.org/10.1080/00031305.2000.10474549</a>.
</div>
<div id="ref-Phillips1977" class="csl-entry">
Phillips, G. D. A. 1977. <span>“Recursions for the Two-Stage
Least-Squares Estimators.”</span> <em>Journal of Econometrics</em> 6:
65–77. <a href="https://doi.org/10.1016/0304-4076(77)90055-0" class="external-link">https://doi.org/10.1016/0304-4076(77)90055-0</a>.
</div>
<div id="ref-Pregibon1981" class="csl-entry">
Pregibon, D. 1981. <span>“Logistic Regression Diagnostics.”</span>
<em>The Annals of Statistics</em> 9: 705–24. <a href="https://doi.org/10.1214/aos/1176345513" class="external-link">https://doi.org/10.1214/aos/1176345513</a>.
</div>
<div id="ref-R" class="csl-entry">
R Core Team. 2020. <em><span>R</span>: A Language and Environment for
Statistical Computing</em>. Vienna, Austria: R Foundation for
Statistical Computing. <a href="https://www.R-project.org/" class="external-link">https://www.R-project.org/</a>.
</div>
<div id="ref-Theil1971" class="csl-entry">
Theil, H. 1971. <em>Principles of Econometrics</em>. New York: John
Wiley &amp; Sons.
</div>
<div id="ref-Tukey1977" class="csl-entry">
Tukey, J. W. 1977. <em>Exploratory Data Analysis</em>. Reading:
Addison-Wesley.
</div>
<div id="ref-VenablesRipley2002" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics
with s</em>. Fourth. New York: Springer. <a href="http://www.stats.ox.ac.uk/pub/MASS4/" class="external-link">http://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
<div id="ref-White1980" class="csl-entry">
White, H. 1980. <span>“A Heteroskedasticity-Consistent Covariance Matrix
Estimator and a Direct Test for Heteroskedasticity.”</span>
<em>Econometrica</em> 48: 817–38. <a href="https://doi.org/10.2307/1912934" class="external-link">https://doi.org/10.2307/1912934</a>.
</div>
<div id="ref-Zeileis2006" class="csl-entry">
Zeileis, A. 2006. <span>“Object-Oriented Computation of Sandwich
Estimators.”</span> <em>Journal of Statistical Software</em> 16 (9):
1–16. <a href="https://doi.org/10.18637/jss.v016.i09" class="external-link">https://doi.org/10.18637/jss.v016.i09</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by John Fox, Christian Kleiber, Achim Zeileis.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

      </footer>
</div>






  </body>
</html>
