<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Diagnostics for 2SLS Regression • ivreg</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="Diagnostics for 2SLS Regression">
<meta property="og:description" content="ivreg">
<meta property="og:image" content="/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">ivreg</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.3-0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../articles/ivreg.html">Get started</a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Diagnostics-for-2SLS-Regression.html">Diagnostics for 2SLS Regression</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/john-d-fox/ivreg/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>Diagnostics for 2SLS Regression</h1>
                        <h4 class="author">John Fox, Christian Kleiber, Achim Zeileis</h4>
            
            <h4 class="date">last modified: 2020-08-04</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/john-d-fox/ivreg/blob/master/vignettes/Diagnostics-for-2SLS-Regression.Rmd"><code>vignettes/Diagnostics-for-2SLS-Regression.Rmd</code></a></small>
      <div class="hidden name"><code>Diagnostics-for-2SLS-Regression.Rmd</code></div>

    </div>

    
    
<div id="introduction" class="section level2">
<h2 class="hasAnchor">
<a href="#introduction" class="anchor"></a>Introduction</h2>
<p>The <strong>ivreg</strong> package extends a variety of standard numeric and graphical regression diagnostics to linear models fit by <em>two-stage least-squares (2SLS)</em> regression, a commonly employed method of instrumental-variables estimation for potentially overidentified structural equations in which there are endogenous regressors. The <code><a href="../reference/ivreg.fit.html">ivreg.fit()</a></code> function in the package computes the 2SLS estimator employing a low-level interface not generally intended for direct use, and returns a list containing quantities that faciliate the computation of various diagnostics. The <code><a href="../reference/ivreg.html">ivreg()</a></code> function provides a user-friendly formula-based interface to <code><a href="../reference/ivreg.fit.html">ivreg.fit()</a></code>.</p>
<p><code><a href="../reference/ivreg.html">ivreg()</a></code> is derived from and supersedes the <code><a href="../reference/ivreg.html">ivreg()</a></code> function in the <strong>AER</strong> package <span class="citation">(Kleiber and Zeileis 2008)</span>, making additional provision for regression diagnostics. The principal subject of this vignette is the rationale for the extension of various standard regression diagnostics to 2SLS and the use of functions in the <strong>ivreg</strong> package to compute them, along with functions in other packages, specifically the base-R <strong>stats</strong> package <span class="citation">(R Core Team 2020)</span> and the <strong>car</strong> and <strong>effects</strong> packages <span class="citation">(Fox and Weisberg 2019)</span>, that work with the <code>"ivreg"</code> objects produced by <code><a href="../reference/ivreg.html">ivreg()</a></code>.</p>
</div>
<div id="review-of-2sls-estimation" class="section level2">
<h2 class="hasAnchor">
<a href="#review-of-2sls-estimation" class="anchor"></a>Review of 2SLS Estimation</h2>
<p>We’ll need some basic results for 2SLS regression to develop diagnostics and so we review the method briefly here. 2SLS regression was invented independently in the 1950s by <span class="citation">Basmann (1957)</span> and Theil <span class="citation">(as cited in Theil 1971)</span>, who took slightly different but equivalent approaches, both described below, to derive the 2SLS estimator.</p>
<p>We want to estimate the linear model <span class="math inline">\(y = X \beta + \varepsilon\)</span>, where <span class="math inline">\(y\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of observations on a response variable, <span class="math inline">\(X\)</span> is an <span class="math inline">\(n \times p\)</span> matrix of regressors, typically with an initial columns of <span class="math inline">\(1\)</span>s for the regression constant, <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(p \times 1\)</span> vector of regression coefficients to be estimated from the data, and <span class="math inline">\(\varepsilon\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of errors assumed to be distributed <span class="math inline">\(N_n(0, \sigma^2 I_n)\)</span> where <span class="math inline">\(N_n\)</span> is the multivariate-normal distribution, <span class="math inline">\(0\)</span> is an <span class="math inline">\(n \times 1\)</span> vector of zeroes, and <span class="math inline">\(I_n\)</span> is the order-<span class="math inline">\(n\)</span> identity matrix. Suppose that some (perhaps all) of the regressors in <span class="math inline">\(X\)</span> are <em>endogenous</em>, in the sense that they are thought not to be independent of <span class="math inline">\(\varepsilon\)</span>. As a consequence, the <em>ordinary least-squares (OLS)</em> estimator <span class="math inline">\(b_{\mathrm{OLS}} = (X^\top X)^{-1} X^\top y\)</span> of <span class="math inline">\(\beta\)</span> is generally biased and inconsistent.</p>
<p>Now suppose that we have another set of <span class="math inline">\(q\)</span> <em>instrumental variables (IVs)</em> <span class="math inline">\(Z\)</span> that are independent of <span class="math inline">\(\epsilon\)</span>, where <span class="math inline">\(q \ge p\)</span>. If <span class="math inline">\(q = p\)</span> we can apply the IVs directly to estimate <span class="math inline">\(\beta\)</span>, but if <span class="math inline">\(q &gt; p\)</span> we have more IVs than we need. Simply discarding IVs would be inefficient, and 2SLS regression is a procedure for reducing the number of IVs to <span class="math inline">\(p\)</span> by combining them in a sensible way.</p>
<p>The <em>first stage</em> of 2SLS regresses all of regressors in the model matrix <span class="math inline">\(X\)</span> on the IVs <span class="math inline">\(Z\)</span> by multivariate ordinary least squares, obtaining the <span class="math inline">\(q \times p\)</span> matrix of regression coefficients <span class="math inline">\(B = (Z^\top Z)^{-1} Z^\top X\)</span>, and the fitted values <span class="math inline">\(\widehat{X} = Z B\)</span>. The columns of <span class="math inline">\(B\)</span> are equivalent to the coefficients produced by separate least-squares regressions of each of the columns of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>. If some of the columns of <span class="math inline">\(X\)</span> are exogenous, then these columns also appear in <span class="math inline">\(Z\)</span>, and consequently the columns of <span class="math inline">\(\widehat{X}\)</span> pertaining to exogenous regressors simply reproduce the corresponding columns of <span class="math inline">\(X\)</span>.</p>
<p>Because the columns of <span class="math inline">\(\widehat{X}\)</span> are linear combinations of the columns of <span class="math inline">\(Z\)</span>, they are (asymptotically) uncorrelated with <span class="math inline">\(\varepsilon\)</span>, making them suitable IVs for estimating the regression equation. This IV step is the <em>second stage</em> of 2SLS in Theil’s approach.</p>
<p>As an alternative, we can obtain exactly the same estimates <span class="math inline">\(b_{\mathrm{2SLS}}\)</span> of <span class="math inline">\(\beta\)</span> by performing an OLS regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(\widehat{X}\)</span>, producing <span class="math inline">\(b_{\mathrm{2SLS}} = (\widehat{X}^\top \widehat{X}) \widehat{X}^\top y\)</span>. This is Basmann’s approach and it motivates the name “2SLS.”</p>
<p>Whether we think of the second stage as IV estimation or OLS regression, we can combine the two stages into a single formula: <span class="math display">\[
b_{\mathrm{2SLS}} = [X^\top Z(Z^\top Z)^{-1} Z^\top X]^{-1} X^\top Z (Z^\top Z)^{-1} Z^\top y
\]</span> This is what the <code>tsls()</code> function in the <strong>sem</strong> package <span class="citation">(Fox, Nie, and Byrnes 2020)</span> does, but from the point of view of developing regression diagnostics, it’s advantageous to compute the 2SLS estimator by two distinct OLS regressions, which is the approach taken by <code><a href="../reference/ivreg.html">ivreg()</a></code>.</p>
</div>
<div id="unusual-data-diagnostics-for-2sls-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#unusual-data-diagnostics-for-2sls-regression" class="anchor"></a>Unusual-Data Diagnostics for 2SLS Regression</h2>
<p>As far as we can tell, diagnostics for regression models fit by 2SLS are a relatively neglected topic, but were addressed briefly by <span class="citation">Belsley, Kuh, and Welsch (1980, 266–268)</span>. Deletion diagnostics directly assess the influence of each case on a fitted regression model by removing the case, refitting the model, and noting how the regression coefficients or other regression outputs, such as the residual standard deviation, change.</p>
<p>Case-deletion diagnostics for influential data can always be obtained by brute-force computation, literally refitting the model with each case removed in turn, but this approach is inefficient and consequently unattractive in large samples. For some classes of statistical models, such as generalized linear models <span class="citation">(e.g., Pregibon 1981)</span>, computationally less demanding approximations to case-deletion diagnostics are available, and for linear models efficient “updating” formulas are available <span class="citation">(as described, e.g., by Belsley, Kuh, and Welsch 1980)</span> that permit the exact computation of case-deletion diagnostics.</p>
<p>As it turns out, and as Belsley, Kuh, and Welsch note, exact updating formulas for 2SLS regression permitting the efficient computation of case-delection statistics were given by <span class="citation">Phillips (1977, Equations 15 and 16)</span>. Phillips’s formulas, reproduced here in our notation (and fixing a couple of small typos in the original), are used in the case-deletion statistics computed in the <strong>ivreg</strong> package: <span class="math display">\[
b_{\mathrm{2SLS}-i} = b_{\mathrm{2SLS}} +A^{-1}g_i
\]</span> where <span class="math inline">\(b_{\mathrm{2SLS}-i}\)</span> is the 2SLS vector of regression coefficients with the <span class="math inline">\(i\)</span>-th case removed, and</p>
<p><span class="math display">\[\begin{align}
A   &amp;= X^\top Z(Z^\top Z)^{-1}Z^\top X \\
g_i &amp;= u_i[(y_i - z_i^\top a) - (y_i - r_i^\top b_{\mathrm{2SLS}})] + (u_i + j_i)(y_i - x_i^\top b_{\mathrm{2SLS}}) \\
u_i &amp;= \frac{1 - x_i^\top A^{-1} x_i}{[1 - c_i + (x_i - r_i)^\top A^{-1} (x_i - r_i)]D_i}(x_i - r_i) 
       + \frac{(x_i - r_i)^\top A^{-1} x_i}{[1 - c_i + (x_i - r_i)^\top A^{-1} (x_i - r_i)]D_i}x_i \\
j_i &amp;= \frac{(x_i - r_i)^\top A^{-1} x_i}{[1 - c_i + (x_i - r_i)^\top A^{-1} (x_i - r_i)]D_i}(x_i - r_i) - \frac{1}{D_i}x_i \\
r_i &amp;= X^\top Z (Z^\top Z)^{-1} z_i \\
c_i &amp;= z_i^\top (Z^\top Z)^{-1} z_i \\
D_i &amp;= 1 - x_i^\top A^{-1} x_i + \frac{[(x_i - r_i)^\top A^{-1} x_i]^2}{1 - c_i + (x_i - r_i)^\top A^{-1}(x_i - r_i)} \\
a   &amp;= (Z^\top Z)^{-1} Z^\top y
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(y_i\)</span> is the value of the response for the <span class="math inline">\(i\)</span>-th case, <span class="math inline">\(x_i^\top\)</span> is the <span class="math inline">\(i\)</span>-th row of the model matrix <span class="math inline">\(X\)</span>, and <span class="math inline">\(z_i^\top\)</span> is the <span class="math inline">\(i\)</span>-th row of the instrumental-variables model matrix <span class="math inline">\(Z\)</span>.</p>
<p>Belsley, Kuh, and Welsch specifically examine (in our notation) the values of <span class="math inline">\(\mathrm{dfbeta}_i = b_{\mathrm{2SLS}} - b_{\mathrm{2SLS}-i}\)</span>. They discuss as well the deleted values of the residual standard deviation <span class="math inline">\(s_{-i}\)</span>. (Belsley, Kuh, and Welsch define the residual variances <span class="math inline">\(s^2\)</span> and <span class="math inline">\(s_{-i}^2\)</span> respectively as the full-sample and deleted residual sums of squares divided by <span class="math inline">\(n\)</span>; in the <strong>ivreg</strong> packages, we divide by the residual degrees of freedom, <span class="math inline">\(n - p\)</span> for the full-sample value of <span class="math inline">\(s^2\)</span> and <span class="math inline">\(n - p - 1\)</span> for the case-deleted values.)</p>
<p>Belsley, Kuh, and Welsch then compute their summary measure of influence on the fitted values (and regression coefficients) <span class="math inline">\(\mathrm{dffits}\)</span> as <span class="math display">\[
\mathrm{dffits}_i = \frac{x_i^\top \mathrm{dfbeta_{i}}}{s_{-i} \sqrt{x_i^\top (\widehat{X}^\top \widehat{X})^{-1} x_i}}
\]</span> where (as before) <span class="math inline">\(x_i^\top\)</span> is the <span class="math inline">\(i\)</span>-th row of the model matrix <span class="math inline">\(X\)</span> and <span class="math inline">\(\widehat{X}\)</span> is the model matrix of second-stage regressors.</p>
<p>Let <span class="math display">\[
H^* = X[X^\top Z(Z^\top Z)^{-1} Z^\top X]^{-1} X^\top Z (Z^\top Z)^{-1} Z^\top
\]</span> represent the <span class="math inline">\(n \times n\)</span> matrix that transforms <span class="math inline">\(y\)</span> into the fitted values, <span class="math inline">\(\widehat{y} = H^* y\)</span>. In OLS regression, the analogous quantity is the <em>hat-matrix</em> <span class="math inline">\(H = X(X^\top X)^{-1}X^\top\)</span>. Belsley, Kuh, and Welsch note that <span class="math inline">\(H^*\)</span>, unlike <span class="math inline">\(H\)</span>, is not an orthogonal-projection matrix, projecting <span class="math inline">\(y\)</span> orthogonally onto the subspace spanned by the columns of <span class="math inline">\(X\)</span>. (They say that <span class="math inline">\(H^*\)</span> isn’t a projection matrix, but that isn’t true: It represents an oblique projection of <span class="math inline">\(y\)</span> onto the subspace spanned by the columns of <span class="math inline">\(X\)</span>.) In particular, although <span class="math inline">\(H^*\)</span>, like <span class="math inline">\(H\)</span>, is idempotent (<span class="math inline">\(H^* = H^* H^*\)</span>) and <span class="math inline">\(\mathrm{trace}(H^*) = p\)</span>, <span class="math inline">\(H^*\)</span>, unlike <span class="math inline">\(H\)</span>, is asymmetric, and thus its diagonal elements can’t be treated as summary measures of leverage, that is, as <em>hatvalues</em>.</p>
<p>Belsley, Kuh, and Welsch recommend simply using the havalues from the second-stage regression. These are the diagonal entries <span class="math inline">\(h_i = h_{ii}\)</span> of <span class="math inline">\(H_2 = \widehat{X}(\widehat{X}^\top \widehat{X})^{-1} \widehat{X}^\top\)</span>. We discuss some alternatives below.</p>
<p>In addition to hatvalues, <span class="math inline">\(\mathrm{dfbeta}\)</span>, <span class="math inline">\(s_{-i}\)</span>, and <span class="math inline">\(\mathrm{dffits}\)</span>, the <strong>ivreg</strong> packages calculates <em>Cook’s distances</em> <span class="math inline">\(D_i\)</span>, which are essentially a slightly differently scaled version of <span class="math inline">\(\mathrm{dffits}\)</span> that uses the overall residual standard deviation <span class="math inline">\(s\)</span> in place of the deleted standard deviations <span class="math inline">\(s_{-i}\)</span>: <span class="math display">\[
D_i = \frac{s_{-i}^2}{s^2} \times \frac{\mathrm{dffits}_i^2}{p}
\]</span></p>
<p>Because they have equal variances and are approximately <span class="math inline">\(t\)</span>-distributed under the normal linear model, <em>studentized residuals</em> are useful for detecting outliers and for addressing the assumption of normally distributed errors. The <strong>ivreg</strong> package defines studentized residuals in analogy to OLS regression as <span class="math display">\[
\mathrm{rstudent}_i = \frac{e_i}{s_{-i} \sqrt{1 - h_i}}
\]</span> where <span class="math inline">\(e_i = y_i - x_i^\top b_{2SLS}\)</span> is the <em>response residual</em> for the <span class="math inline">\(i\)</span>-th case.</p>
<p>As mentioned, <span class="citation">Belsley, Kuh, and Welsch (1980)</span> recommend using hatvalues from the second-stage regression. That’s a reasonable choice and the default in the <strong>ivreg</strong> package, but it risks missing cases that have high leverage in the first-stage but not the second-stage regression. Let <span class="math inline">\(h_i^{(1)}\)</span> represent the hatvalues from the first stage and <span class="math inline">\(h_i^{(2)}\)</span> those from the second stage. If the model includes an intercept, both sets of hatvalues are bounded by <span class="math inline">\(1/n\)</span> and <span class="math inline">\(1\)</span>, but the average hatvalue in the first stage is <span class="math inline">\(q/n\)</span> while the average in the second stage is <span class="math inline">\(p/n\)</span>. To make the hatvalues from the two stages comparable, we divide each by its average, <span class="math inline">\(h^{(1*)}_i = \frac{h_i^{(1)}}{q/n}\)</span> and <span class="math inline">\(h^{(2*)}_i = \frac{h_i^{(2)}}{p/n}\)</span>. Then we can define the two-stages hatvalue either as the (rescaled) larger of the two for each case, <span class="math inline">\(h_i = (p/n) \times \max \left( h^{(1*)}_i, h^{(2*)}_i \right)\)</span>, or as their (rescaled) geometric mean, <span class="math inline">\(h_i = (p/n) \times \sqrt{h^{(1*)}_i \times h^{(2*)}_i}\)</span>. The <strong>ivreg</strong> package provides both of these options.</p>
<div id="unusual-data-diagnosics-in-the-ivreg-package" class="section level3">
<h3 class="hasAnchor">
<a href="#unusual-data-diagnosics-in-the-ivreg-package" class="anchor"></a>Unusual-Data Diagnosics in the <strong>ivreg</strong> Package</h3>
<p>The <strong>ivreg</strong> package implements unusual-data diagnostics for 2SLS regression (i.e., class <code>"ivreg"</code> objects produced by <code><a href="../reference/ivreg.html">ivreg()</a></code>) as methods for various generic functions in the <strong>stats</strong> and <strong>car</strong> packages; these functions include <code><a href="https://rdrr.io/r/stats/influence.measures.html">cooks.distance()</a></code>, <code><a href="https://rdrr.io/r/stats/influence.measures.html">dfbeta()</a></code>, <code><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues()</a></code>, <code><a href="https://rdrr.io/r/stats/lm.influence.html">influence()</a></code>, and <code><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent()</a></code> in <strong>stats</strong>, and <code><a href="https://rdrr.io/pkg/car/man/avPlots.html">avPlot()</a></code> and <code><a href="https://rdrr.io/pkg/car/man/qqPlot.html">qqPlot()</a></code> in <strong>car</strong>. In particular, <code><a href="../reference/ivregDiagnostics.html">influence.ivreg()</a></code> returns an object containing several diagnostic statistics, and it is thus more efficient to use the <code><a href="https://rdrr.io/r/stats/lm.influence.html">influence()</a></code> function than to compute the various diagnostics separately. Methods provided for class <code>"influence.ivreg"</code> objects include <code><a href="https://rdrr.io/r/stats/influence.measures.html">cooks.distance()</a></code>, <code><a href="https://rdrr.io/r/stats/influence.measures.html">dfbeta()</a></code>, <code><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues()</a></code>, <code><a href="https://rdrr.io/pkg/car/man/qqPlot.html">qqPlot()</a></code>, and <code><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent()</a></code>.</p>
<p>The package also provides methods for various standard R regression-model generics, including <code><a href="https://rdrr.io/r/stats/anova.html">anova()</a></code> (for model comparison), <code>predicted()</code> for computing predicted values, <code><a href="https://rdrr.io/r/stats/model.matrix.html">model.matrix()</a></code> (for the model or for the first- or second-stage regression), <code><a href="https://rdrr.io/r/base/print.html">print()</a></code>, <code><a href="https://rdrr.io/r/stats/residuals.html">residuals()</a></code> (of several kinds), <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code>, <code><a href="https://rdrr.io/r/stats/update.html">update()</a></code>, and <code><a href="https://rdrr.io/r/stats/vcov.html">vcov()</a></code>. The <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> method makes provision for a user-specified coefficient covariance matrix or for a function to compute the coefficient covariance matrix, such as <code>sandwich()</code> in the <strong>sandwich</strong> package, to compute robust coefficient covariances. The latter is supported by methods for the <code>bread()</code> and <code>estfun()</code> generics defined in <strong>sandwich</strong>.</p>
</div>
<div id="unusual-data-diagnostics-an-example" class="section level3">
<h3 class="hasAnchor">
<a href="#unusual-data-diagnostics-an-example" class="anchor"></a>Unusual Data Diagnostics: An Example</h3>
<p>The <strong>ivreg</strong> package contains the <code>Kmenta</code> data set, used in <span class="citation">Kmenta (1986, Ch. 13)</span> to illustrate estimation (by 2SLS and other methods) of a linear simultaneous equation econometric model. The data, which are partly contrived, represent an annual time series for the U.S. economy from 1922 to 1941, with the following variables:</p>
<ul>
<li>
<code>Q</code>, food consumption per capita</li>
<li>
<code>P</code>, ratio of food prices to general consumer prices</li>
<li>
<code>D</code>, disposible income in constant dollars</li>
<li>
<code>F</code>, ratio of preceding year’s prices received by famers to general consumer prices</li>
<li>
<code>A</code>, time in years</li>
</ul>
<p>The data set is small and so we can examine it in its entirety:</p>
<div class="sourceCode" id="cb1"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="st">"ivreg"</span>)
<span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span>(<span class="st">"Kmenta"</span>, <span class="kw">package</span> <span class="kw">=</span> <span class="st">"ivreg"</span>)
<span class="no">Kmenta</span></pre></body></html></div>
<pre><code>##            Q       P     D     F  A
## 1922  98.485 100.323  87.4  98.0  1
## 1923  99.187 104.264  97.6  99.1  2
## 1924 102.163 103.435  96.7  99.1  3
## 1925 101.504 104.506  98.2  98.1  4
## 1926 104.240  98.001  99.8 110.8  5
## 1927 103.243  99.456 100.5 108.2  6
## 1928 103.993 101.066 103.2 105.6  7
## 1929  99.900 104.763 107.8 109.8  8
## 1930 100.350  96.446  96.6 108.7  9
## 1931 102.820  91.228  88.9 100.6 10
## 1932  95.435  93.085  75.1  81.0 11
## 1933  92.424  98.801  76.9  68.6 12
## 1934  94.535 102.908  84.6  70.9 13
## 1935  98.757  98.756  90.6  81.4 14
## 1936 105.797  95.119 103.1 102.3 15
## 1937 100.225  98.451 105.1 105.0 16
## 1938 103.522  86.498  96.4 110.5 17
## 1939  99.929 104.016 104.4  92.5 18
## 1940 105.223 105.769 110.7  89.3 19
## 1941 106.232 113.490 127.1  93.0 20</code></pre>
<p>Kmenta estimated the following two-equation model, with the first equation representing demand and the second supply: <span class="math display">\[\begin{align}
Q &amp;= \beta_{10} + \beta_{11} P + \beta_{12} D + \varepsilon_1 \\
Q &amp;= \beta_{20} + \beta_{21} P + \beta_{22} F + \beta_{23} A + \varepsilon_2
\end{align}\]</span> The variables <span class="math inline">\(D\)</span>, <span class="math inline">\(F\)</span>, and <span class="math inline">\(A\)</span> are taken as exogenous, as of course is the constant regressor (a columns of <span class="math inline">\(1\)</span>s), and <span class="math inline">\(P\)</span> in both structural equations is an endogenous explanatory variable. Because there are four instrumental variables available, the first structural equation, which has three coefficients, is over-identified, while the second structural equation, with four coefficients, is just-identified.</p>
<p>The values of the exogenous variables are real, while those of the endogenous variables were generated (i.e., simulated) by Kmenta according to the model, with the following assumed values of the parameters:</p>
<p><span class="math display">\[\begin{align}
Q &amp;= 96.5 - 0.25 P + 0.30 D + \varepsilon_1 \\
Q &amp;= 62.5 + 0.15 P + 0.20 F + 0.36 A + \varepsilon_2
\end{align}\]</span></p>
<p>Solving the structural equations for the endogenous variables <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> produces the <em>reduced form</em> of the model</p>
<p><span class="math display">\[\begin{align}
Q &amp;= 75.25 + 11.25 D + 0.125 F + 0.225 A + \nu_1\\
P &amp;= 85.00 + 0.75 D - 0.50 F - 0.90 A + \nu_2
\end{align}\]</span></p>
<p>Kmenta independently sampled 20 values of <span class="math inline">\(\delta_1\)</span> and <span class="math inline">\(\delta_2\)</span>, each from <span class="math inline">\(N(0, 1)\)</span>, and then set <span class="math inline">\(\nu_1 = 2 \delta_1\)</span> and <span class="math inline">\(\nu_2 = -0.5 \nu_1 + \delta_2\)</span>.</p>
<p>The structural equations are estimated as follows by the <code><a href="../reference/ivreg.html">ivreg()</a></code> function <span class="citation">(compare Kmenta 1986, 686)</span>:</p>
<div class="sourceCode" id="cb3"><html><body><pre class="r"><span class="no">deq</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/ivreg.html">ivreg</a></span>(<span class="no">Q</span> ~ <span class="no">P</span> + <span class="no">D</span>, ~ <span class="no">D</span> + <span class="no">F</span> + <span class="no">A</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">Kmenta</span>)     <span class="co"># demand equation</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="no">deq</span>)</pre></body></html></div>
<pre><code>## 
## Call:
## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4305 -1.2432 -0.1895  1.5762  2.4920 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 94.63330    7.92084  11.947 1.08e-09 ***
## P           -0.24356    0.09648  -2.524   0.0218 *  
## D            0.31399    0.04694   6.689 3.81e-06 ***
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   2  16    88.025 2.32e-09 ***
## Wu-Hausman         1  16    11.422  0.00382 ** 
## Sargan             1  NA     2.983  0.08414 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.966 on 17 degrees of freedom
## Multiple R-Squared: 0.7548,  Adjusted R-squared: 0.726 
## Wald test: 23.81 on 2 and 17 DF,  p-value: 1.178e-05</code></pre>
<div class="sourceCode" id="cb5"><html><body><pre class="r"><span class="no">seq</span> <span class="kw">&lt;-</span> <span class="fu"><a href="../reference/ivreg.html">ivreg</a></span>(<span class="no">Q</span> ~ <span class="no">P</span> + <span class="no">F</span> + <span class="no">A</span>, ~ <span class="no">D</span> + <span class="no">F</span> + <span class="no">A</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">Kmenta</span>) <span class="co"># supply equation</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="no">seq</span>)</pre></body></html></div>
<pre><code>## 
## Call:
## ivreg(formula = Q ~ P + F + A | D + F + A, data = Kmenta)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8724 -1.2593  0.6415  1.4745  3.4865 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 49.53244   12.01053   4.124 0.000795 ***
## P            0.24008    0.09993   2.402 0.028785 *  
## F            0.25561    0.04725   5.410 5.79e-05 ***
## A            0.25292    0.09966   2.538 0.021929 *  
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   1  16    256.34 2.86e-11 ***
## Wu-Hausman         1  15     36.14 2.38e-05 ***
## Sargan             0  NA        NA       NA    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.458 on 16 degrees of freedom
## Multiple R-Squared: 0.6396,  Adjusted R-squared: 0.572 
## Wald test:  10.7 on 3 and 16 DF,  p-value: 0.0004196</code></pre>
<p>By default, <code><a href="https://rdrr.io/r/base/summary.html">summary()</a></code> prints the results of three “diagnostic” tests for 2SLS regression. These tests (which can be suppressed by setting the argument <code>diagnostics=FALSE</code>) are not the focus of the vignette and so we’ll comment on them only briefly:</p>
<ul>
<li><p>A good instrumental variable is highly correlated with one or more of the explanatory variables while remaining uncorrelated with the errors. If an endogenous regressor is only weakly related to the instrumental variables, then its coefficient will be estimated imprecisely. We hope for a large test statistic and small <span class="math inline">\(p\)</span>-value in the diagnostic test for weak instruments, as is the case for both regression equations in the Kmenta model.</p></li>
<li><p>Applied to 2SLS regression, the Wu–Hausman test is a test of <em>endogenity</em>. If all of the regressors are exogenous, then both the OLS and 2SLS estimators are consistent, and the OLS estimator is more efficient, but if one or more regressors are endogenous, then the OLS estimator is inconsistent. A large test statistic and small <span class="math inline">\(p\)</span>-value, as in the example, suggests that the OLS estimator is inconsistent and the 2SLS estimator is therefore to be preferred.</p></li>
<li><p>The Sargan test is a test of overidentification. That is, in an overidentified regression equation, where there are more instrumental variables than coefficients to estimate, as in Kmenta’s demand equation, it’s possible that the instrumental variables provide conflicting information about the values of the coefficients. A large test statistic and small <span class="math inline">\(p\)</span>-value for the Sargan test suggest, therefore, that the model is misspecified. In the example, we obtain a moderately small <span class="math inline">\(p\)</span>-value of 0.084 by chance even though we know (by the manner in which the data were constructed) that the demand equation is correct. The Sargan test is inapplicable to a just-identified regression equation, with an equal number of instrumental variables and coefficients, as in Kmenta’s supply equation.</p></li>
</ul>
<p>Several methods for class <code>"lm"</code> objects work properly with the objects produced by <code><a href="../reference/ivreg.html">ivreg()</a></code>. For example, the <code><a href="https://rdrr.io/r/base/plot.html">plot()</a></code> method for <code>"ivreg"</code> objects invokes the corresponding <code>"lm"</code> method and produces interpretable plots, here for the 2SLS fit for the demand equation in Kmenta’s model:</p>
<div class="sourceCode" id="cb7"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/graphics/par.html">par</a></span>(<span class="kw">mfrow</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fl">2</span>, <span class="fl">2</span>))
<span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">deq</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-4-1.png" width="768"></p>
<p>In this case, however, we prefer the versions of these diagnostic graphs described below, in this and subsequent sections.</p>
<p>As we mentioned, the <code>Kmenta</code> data are partly contrived by simulating the model, and so it’s probably not surprising that the data are well behaved. For example, a <em>QQ plot</em> of studentized residuals and an <em>“influence plot”</em> of hatvalues, studentized residuals, and Cook’s distances for the first structural equation are both unremarkable, except for a couple of high-leverage but in-line cases:</p>
<div class="sourceCode" id="cb8"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="st">"car"</span>) <span class="co"># for diagnostic generic functions</span></pre></body></html></div>
<pre><code>## Loading required package: carData</code></pre>
<div class="sourceCode" id="cb10"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/qqPlot.html">qqPlot</a></span>(<span class="no">deq</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-5-1.png" width="384"></p>
<pre><code>## 1937 1929 
##   16    8</code></pre>
<div class="sourceCode" id="cb12"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/influencePlot.html">influencePlot</a></span>(<span class="no">deq</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-6-1.png" width="384"></p>
<pre><code>##         StudRes        Hat      CookD
## 1929 -1.7359357 0.09079703 0.06956671
## 1933 -1.3686682 0.26453459 0.21973049
## 1937 -2.0995532 0.13849570 0.17147564
## 1938 -0.2010944 0.39711512 0.01508349
## 1941 -0.4505155 0.46498004 0.05257374</code></pre>
<p>The circles in the influence plot have areas proportional to Cook’s D, the horizontal lines are drawn at 0 and <span class="math inline">\(\pm 2\)</span> on the studentized residuals scale (the horizontal line at <span class="math inline">\(\mathrm{rstudent} = 2\)</span> is off the graph), and the vertical lines are at <span class="math inline">\(2 \times \bar{h}\)</span> and <span class="math inline">\(3 \times \bar{h}\)</span>. We invite the reader to repeat these graphs, and the example below, for the second structural equation.</p>
<p>To generate a more interesting example, we’ll change the value of <span class="math inline">\(Q\)</span> for the high-leverage 20th case (i.e, for 1941) from <span class="math inline">\(Q_{20} = 106.232\)</span> to <span class="math inline">\(Q_{20} = 95\)</span>, a value that’s well within the range of <span class="math inline">\(Q\)</span> in the data but out of line with the rest of the data:</p>
<div class="sourceCode" id="cb14"><html><body><pre class="r"><span class="no">Kmenta1</span> <span class="kw">&lt;-</span> <span class="no">Kmenta</span>
<span class="no">Kmenta1</span>[<span class="fl">20</span>, <span class="st">"Q"</span>] <span class="kw">&lt;-</span> <span class="fl">95</span></pre></body></html></div>
<p>Then repeating the 2SLS fit for the first structural equation and comparing the results to those for the uncorrupted data reveals substantial change in the regression coefficients:</p>
<div class="sourceCode" id="cb15"><html><body><pre class="r"><span class="no">deq1</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span>(<span class="no">deq</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">Kmenta1</span>)
<span class="fu"><a href="https://rdrr.io/pkg/car/man/compareCoefs.html">compareCoefs</a></span>(<span class="no">deq</span>, <span class="no">deq1</span>)</pre></body></html></div>
<pre><code>## Calls:
## 1: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)
## 2: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1)
## 
##             Model 1 Model 2
## (Intercept)   94.63  117.96
## SE             7.92   11.64
##                            
## P           -0.2436 -0.4054
## SE           0.0965  0.1417
##                            
## D            0.3140  0.2351
## SE           0.0469  0.0690
## </code></pre>
<p>The problematic 20th case (the year 1941) is clearly revealed by unusual-data regression diagnostics:</p>
<div class="sourceCode" id="cb17"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/qqPlot.html">qqPlot</a></span>(<span class="no">deq1</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-9-1.png" width="384"></p>
<pre><code>## 1941 1940 
##   20   19</code></pre>
<div class="sourceCode" id="cb19"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/outlierTest.html">outlierTest</a></span>(<span class="no">deq1</span>)</pre></body></html></div>
<pre><code>##       rstudent unadjusted p-value Bonferroni p
## 1941 -4.599583         0.00029602    0.0059204</code></pre>
<div class="sourceCode" id="cb21"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/influencePlot.html">influencePlot</a></span>(<span class="no">deq1</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-11-1.png" width="384"></p>
<pre><code>##         StudRes       Hat     CookD
## 1933 -1.4737565 0.2645346 0.2447875
## 1938 -0.9139638 0.3971151 0.2269833
## 1940  1.6021281 0.1280020 0.1155278
## 1941 -4.5995825 0.4649800 2.8361307</code></pre>
<div class="sourceCode" id="cb23"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/avPlots.html">avPlots</a></span>(<span class="no">deq1</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-12-1.png" width="768"></p>
<p>Removing the 20th case produces estimated coefficients close to those for the uncorrupted data:</p>
<div class="sourceCode" id="cb24"><html><body><pre class="r"><span class="no">deq1.20</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span>(<span class="no">deq1</span>, <span class="kw">subset</span> <span class="kw">=</span> -<span class="fl">20</span>)
<span class="fu"><a href="https://rdrr.io/pkg/car/man/compareCoefs.html">compareCoefs</a></span>(<span class="no">deq</span>, <span class="no">deq1</span>, <span class="no">deq1.20</span>)</pre></body></html></div>
<pre><code>## Calls:
## 1: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)
## 2: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1)
## 3: ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta1, subset = -20)
## 
##             Model 1 Model 2 Model 3
## (Intercept)   94.63  117.96   92.42
## SE             7.92   11.64    9.67
##                                    
## P           -0.2436 -0.4054 -0.2300
## SE           0.0965  0.1417  0.1047
##                                    
## D            0.3140  0.2351  0.3233
## SE           0.0469  0.0690  0.0527
## </code></pre>
<p>The standard errors of the estimated coefficients are larger than they were originally because we now have 19 rather than 20 cases and because the variation of the explanatory variables is reduced.</p>
<p>It’s of some interest to discover whether the three definitions of hatvaues make a practical difference to this example. A scatterplot matrix for the three kinds of hatvalues suggests that they all produce similar results:</p>
<div class="sourceCode" id="cb26"><html><body><pre class="r"><span class="no">H</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span>(<span class="no">deq1</span>), <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span>(<span class="no">deq1</span>, <span class="kw">type</span><span class="kw">=</span><span class="st">"both"</span>),
           <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">hatvalues</a></span>(<span class="no">deq1</span>, <span class="kw">type</span><span class="kw">=</span><span class="st">"maximum"</span>))
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">H</span>) <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"stage2"</span>, <span class="st">"geom.mean"</span>, <span class="st">"maximum"</span>)
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span>(<span class="no">H</span>)</pre></body></html></div>
<pre><code>##          stage2  geom.mean    maximum
## 1922 0.10349313 0.12269459 0.14545857
## 1923 0.11215042 0.12972476 0.15005306
## 1924 0.08882553 0.10233878 0.11790784
## 1925 0.09515432 0.10207539 0.10949987
## 1926 0.06166289 0.07959715 0.10274748
## 1927 0.05684346 0.06794727 0.08122009</code></pre>
<div class="sourceCode" id="cb28"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html">scatterplotMatrix</a></span>(<span class="no">H</span>, <span class="kw">smooth</span><span class="kw">=</span><span class="fl">FALSE</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-14-1.png" width="576"></p>
<p>Finally, let’s verify that the deletion diagnostics are correctly computed:</p>
<div class="sourceCode" id="cb29"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">dfbeta</a></span>(<span class="no">deq1</span>)[<span class="fl">20</span>, ], <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(<span class="no">deq1</span>) - <span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span>(<span class="no">deq1.20</span>))</pre></body></html></div>
<pre><code>##                    [,1]        [,2]
## (Intercept) 25.53936742 25.53936742
## P           -0.17547231 -0.17547231
## D           -0.08827334 -0.08827334</code></pre>
<div class="sourceCode" id="cb31"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.influence.html">influence</a></span>(<span class="no">deq1</span>)$<span class="no">sigma</span>[<span class="fl">20</span>], <span class="fu"><a href="https://rdrr.io/r/stats/sigma.html">sigma</a></span>(<span class="no">deq1.20</span>))</pre></body></html></div>
<pre><code>##     1941          
## 2.028434 2.028434</code></pre>
</div>
</div>
<div id="nonlinearity-diagnostics" class="section level2">
<h2 class="hasAnchor">
<a href="#nonlinearity-diagnostics" class="anchor"></a>Nonlinearity Diagnostics</h2>
<p>The theoretical properties of <em>component-plus-residual plots</em> as nonlinearity diagnostics were systematically explored by <span class="citation">Cook (1993)</span> and <span class="citation">Cook and Croos-Dabrera (1998)</span>. Following these authors and focusing on the explanatory variable <span class="math inline">\(x_1\)</span>, let’s assume that the partial relationship of the response <span class="math inline">\(y\)</span> to <span class="math inline">\(x_1\)</span> is potentially nonlinear, as represented by the partial regression function <span class="math inline">\(f(x_1)\)</span>, and that the partial relationships of <span class="math inline">\(y\)</span> to the other <span class="math inline">\(x\)</span>s are linear, so that an accurate model for the data is: <span class="math display">\[
E(y) = \alpha + f(x_1) + \beta_2 x_2 + \cdots + \beta_k x_k 
\]</span></p>
<p>We don’t know <span class="math inline">\(f()\)</span> and so instead fit the <em>working model</em> <span class="math display">\[
E(y) = \alpha^\prime + \beta_1^\prime x_1 + \beta_2^\prime x_2 + \cdots + \beta_k^\prime x_k 
\]</span> in our case by 2SLS regression, obtaining estimated regression coefficients <span class="math inline">\(a^\prime, b_1^\prime, b_2^\prime, \ldots, b_k^\prime\)</span>. Cook and Croos-Dabrera’s work shows that as long as the regression estimator is consistant and the <span class="math inline">\(x\)</span>s are linearly related, the partial residuals <span class="math inline">\(b^\prime_1 x_1 + e\)</span> can be plotted and smoothed against <span class="math inline">\(x_1\)</span> to visualize an estimate of <span class="math inline">\(f()\)</span>, where <span class="math inline">\(e = y - (a^\prime + b_1^\prime x_1 + b_2^\prime x_2 + \cdots b_k^\prime x_k)\)</span> are the response residuals. In practice, the component-plus-residual plot can break down as an accurate representation of <span class="math inline">\(f()\)</span> if there are strong nonlinear relationships between <span class="math inline">\(x_1\)</span> and the other <span class="math inline">\(x\)</span>s or if <span class="math inline">\(y\)</span> is nonlinearly related to another <span class="math inline">\(x\)</span> that is correlated with <span class="math inline">\(x_1\)</span>.</p>
<p><span class="citation">Fox and Weisberg (2018)</span> extend component-plus-residual plots to more complex regression models, which can, for example, include interactions, by adding partial residuals to <em>predictor effect plots</em>. These graphs also can be applied to linear models fit by 2SLS regression.</p>
<div id="diagnosing-nonlinearity-an-example" class="section level3">
<h3 class="hasAnchor">
<a href="#diagnosing-nonlinearity-an-example" class="anchor"></a>Diagnosing Nonlinearity: An Example</h3>
<p>We turn once more to the demand equation for Kmenta’s data and model to illustrate component-plus-residual plots, and once more the data are well behaved. An <code>"ivreg"</code> method is provided for the <code><a href="https://rdrr.io/pkg/car/man/crPlots.html">crPlot()</a></code> function in the <strong>car</strong> package. In particular, <code><a href="https://rdrr.io/pkg/car/man/crPlots.html">crPlots()</a></code> constructs component-plus-residual plots for all of the numeric explanatory variables in an additive regression equation. For example,</p>
<div class="sourceCode" id="cb33"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html">crPlots</a></span>(<span class="no">deq</span>, <span class="kw">smooth</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">span</span><span class="kw">=</span><span class="fl">1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-17-1.png" width="768"></p>
<p>We set a large <em>span</em> for the <em>loess smoother</em> <span class="citation">(Cleveland, Grosse, and Shyu 1992)</span> in the plot because there are only <span class="math inline">\(n = 20\)</span> cases in the <code>Kmenta</code> data set. The default value of the span is <span class="math inline">\(2/3\)</span>. In each panel, the loess smooth, given by the magenta line, closely matches the least-squares line, given by the broken blue line, which represents the fitted regression plane viewed edge-on in the direction of the <em>focal explanatory variable</em>, <span class="math inline">\(P\)</span> on the left and <span class="math inline">\(D\)</span> on the right. Both partial relationships therefore appear to be linear.</p>
<p>CERES plots <span class="citation">(Cook 1993)</span>, implemented in the <code><a href="https://rdrr.io/pkg/car/man/ceresPlots.html">ceresPlots()</a></code> function in the <strong>car</strong> package, are a version of component-plus-residuals plots that use smoothers rather than linear regression and that therefore are more robust with respect to nonlinear relationships among the predictors. In most applications, component-plus-residuals and CERES plots produce similar results, and that’s the case here:</p>
<div class="sourceCode" id="cb34"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/ceresPlots.html">ceresPlots</a></span>(<span class="no">deq</span>, <span class="kw">smooth</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">span</span><span class="kw">=</span><span class="fl">1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-18-1.png" width="768"></p>
<p><code><a href="https://rdrr.io/pkg/car/man/crPlots.html">crPlots()</a></code> and <code><a href="https://rdrr.io/pkg/car/man/ceresPlots.html">ceresPlots()</a></code> work only for additive models; the <code><a href="https://rdrr.io/pkg/effects/man/predictorEffects.html">predictorEffects()</a></code> function in the <strong>effects</strong> package plots partial residuals for more complex models. In the current example, which is an additive model, we get essentially the same graphs as before, except for the scaling of the <span class="math inline">\(y\)</span> axis:</p>
<div class="sourceCode" id="cb35"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/library.html">library</a></span>(<span class="st">"effects"</span>)</pre></body></html></div>
<pre><code>## Registered S3 methods overwritten by 'lme4':
##   method                          from
##   cooks.distance.influence.merMod car 
##   influence.merMod                car 
##   dfbeta.influence.merMod         car 
##   dfbetas.influence.merMod        car</code></pre>
<pre><code>## lattice theme set by effectsTheme()
## See ?effectsTheme for details.</code></pre>
<div class="sourceCode" id="cb38"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="fu"><a href="https://rdrr.io/pkg/effects/man/predictorEffects.html">predictorEffects</a></span>(<span class="no">deq</span>, <span class="kw">residuals</span><span class="kw">=</span><span class="fl">TRUE</span>),
     <span class="kw">partial.residuals</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">span</span><span class="kw">=</span><span class="fl">1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-19-1.png" width="768"></p>
<p>The shaded blue regions in the predictor effect plots represent pointwise 95% confidence envelopes around the fitted partial-regression lines.</p>
<p>Suppose, however, that we fit the wrong model to the data:</p>
<div class="sourceCode" id="cb39"><html><body><pre class="r"><span class="no">deq2</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span>(<span class="no">deq</span>, <span class="no">.</span> ~ <span class="fu"><a href="https://rdrr.io/r/base/AsIs.html">I</a></span>((<span class="no">P</span> - <span class="fl">85</span>)^<span class="fl">4</span>/<span class="fl">10</span>^<span class="fl">5</span>) + <span class="no">D</span>)
<span class="fu"><a href="https://rdrr.io/pkg/car/man/crPlots.html">crPlots</a></span>(<span class="no">deq2</span>, <span class="kw">smooth</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">span</span><span class="kw">=</span><span class="fl">1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-20-1.png" width="768"></p>
<p>Because the ratio <span class="math inline">\(\max(P)/\min(P) = 113.49/86.50 = 1.3\)</span> is not much larger than 1, we subtracted a number slightly smaller than <span class="math inline">\(\min(P)\)</span> from <span class="math inline">\(P\)</span> prior to raising the variable to the 4th power to induce substantial nonlinearity into the fitted partial regression curve. The resulting component-plus-residual plot for the transformed <span class="math inline">\(P\)</span> clearly reflects the resulting lack of fit, while the plot for <span class="math inline">\(D\)</span> is still reasonably linear.</p>
<p>Predictor effect plots with partial residuals show a different view of the same situation by placing <code>P</code> rather than the transformed <code>P</code> on the horizontal axis, and revealing that the fitted nonlinear partial regression function fails to capture the linear pattern of the data:</p>
<div class="sourceCode" id="cb40"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="fu"><a href="https://rdrr.io/pkg/effects/man/predictorEffects.html">predictorEffects</a></span>(<span class="no">deq2</span>, <span class="kw">residuals</span><span class="kw">=</span><span class="fl">TRUE</span>),
     <span class="kw">partial.residuals</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">span</span><span class="kw">=</span><span class="fl">1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-21-1.png" width="768"></p>
<p>Recall that the blue lines represent the fitted model and the magenta lines are for the smoothed partial residuals; discrepancy between the two lines is indicative of lack of fit.</p>
</div>
</div>
<div id="nonconstant-error-variance" class="section level2">
<h2 class="hasAnchor">
<a href="#nonconstant-error-variance" class="anchor"></a>Nonconstant Error Variance</h2>
<p>Standard least-squares nonconstant variance (“heteroscedasticity”) diagnostics extend straightforwardly to 2SLS regression. We can, for example, plot studentized residuals versus fitted values to discern a tendency for the variability of the former to change (typically to increase) with the level of the latter. For the demand equation in Kmenta’s model,</p>
<div class="sourceCode" id="cb41"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span>(<span class="no">deq</span>), <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent</a></span>(<span class="no">deq</span>))
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span>(<span class="kw">h</span><span class="kw">=</span><span class="fl">0</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-22-1.png" width="384"></p>
<p>which seems unproblematic.</p>
<p>A variation of this graph, suggested by <span class="citation">Fox (2016)</span>, adapts Tukey’s <em>spread-level plot</em> <span class="citation">(Tukey 1977)</span> to graph the log of the absolute studentized residuals versus the log of the fitted values, assuming that the latter are positive. If a line fit to the plot has slope <span class="math inline">\(b\)</span>, then a variance-stablilizing power transformation is given by <span class="math inline">\(y^\lambda = y^{1 - b}\)</span>. Thus if <span class="math inline">\(b &gt; 0\)</span>, the suggested transformation is <em>down</em> Tukey’s ladder of powers and roots, with, for example, <span class="math inline">\(\lambda = 1 - b = 1/2\)</span> representing the square-root transformation, <span class="math inline">\(\lambda = 1 - b = 0\)</span> the log transformation, and so on. For Kmenta’s model, we have</p>
<div class="sourceCode" id="cb42"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/spreadLevelPlot.html">spreadLevelPlot</a></span>(<span class="no">deq</span>, <span class="kw">smooth</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span>(<span class="kw">span</span><span class="kw">=</span><span class="fl">1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-23-1.png" width="384"></p>
<pre><code>## 
## Suggested power transformation:  -2.44685</code></pre>
<p>which suggests a slight tendency of spread to increase with level. The transformation <span class="math inline">\(\lambda = - 2.45\)</span> seems strong, until we notice that the values of <span class="math inline">\(Q\)</span> are far from 0, and that the ratio of the largest to smallest values <span class="math inline">\(Q_{\mathrm{max}}/Q_{\mathrm{min}} = 106.23/92.42 = 1.15\)</span> is close to 1, so that <span class="math inline">\(Q^{-2.45}\)</span> is nearly a linear transformation of <span class="math inline">\(Q\)</span>—that is, effectively no transformation at all:</p>
<div class="sourceCode" id="cb44"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span>(<span class="no">Kmenta</span>, <span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">Q</span>, <span class="no">Q</span>^<span class="fl">2.5</span>))
<span class="fu"><a href="https://rdrr.io/r/graphics/abline.html">abline</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span>(<span class="no">Q</span>^<span class="fl">2.5</span> ~ <span class="no">Q</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">Kmenta</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-24-1.png" width="384"></p>
<p>A common score test for nonconstant error variance in least-squares regression, suggested by <span class="citation">Breusch and Pagan (1979)</span>, is based on the model <span class="math display">\[
V(\varepsilon) = g(\gamma_0 + \gamma_1 z_1 + \cdots + \gamma_s z_s) 
\]</span> where the function <span class="math inline">\(g()\)</span> is unspecified and the variables <span class="math inline">\(z_1, \ldots, z_s\)</span> are predictors of the error variance. In the most common application, independently proposed by <span class="citation">Cook and Weisberg (1983)</span>, there is one <span class="math inline">\(z\)</span>, the fitted values <span class="math inline">\(\widehat{y}\)</span> from the regression, although it is also common to use the regressors <span class="math inline">\(x\)</span> from the primary regression as <span class="math inline">\(z\)</span>s. The test is implemented by regressing the squared standardized residuals <span class="math inline">\(e_i^2/\widehat{\sigma}^2\)</span> on the <span class="math inline">\(z\)</span>s, where <span class="math inline">\(\widehat{\sigma}^2 = \sum e_i^2/n\)</span>. The regression sum of squares for this auxiliary regression divided by 2 is then asymptotically distributed as <span class="math inline">\(\chi^2_s\)</span> under the null hypothesis of constant error variance.</p>
<p>The Breusch-Pagan/Cook-Weisberg test is easily adaptable to 2SLS regression, as implemented by the <code><a href="https://rdrr.io/pkg/car/man/ncvTest.html">ncvTest()</a></code> function in the <strong>car</strong> package. For Kmenta’s demand equation:</p>
<div class="sourceCode" id="cb45"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html">ncvTest</a></span>(<span class="no">deq</span>)</pre></body></html></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 0.2390325, Df = 1, p = 0.62491</code></pre>
<div class="sourceCode" id="cb47"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html">ncvTest</a></span>(<span class="no">deq</span>, <span class="kw">var</span> <span class="kw">=</span> ~ <span class="no">P</span> + <span class="no">D</span>)</pre></body></html></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ P + D 
## Chisquare = 0.2392964, Df = 2, p = 0.88723</code></pre>
<p>Here, the first test is against the fitted values and the second more general test is against the explanatory variables in the demand equation; the <span class="math inline">\(p\)</span>-values for both tests are large, suggesting little evidence against the hypothesis of constant variance.</p>
<p>Remedies for nonconstant variance in 2SLS regression are similar to those in least-squares regression:</p>
<ul>
<li><p>We’ve already suggested that if the error variance increases (or decreases) with the level of the response, and if the response is positive, then we might be able to stabilize the error variance by power-transforming the response.</p></li>
<li><p>If, alternatively, we know the variance of the errors up to a constant of proportionality, then we can use inverse-variance weights for the 2SLS estimator. The <code><a href="../reference/ivreg.html">ivreg()</a></code> function supports weighted 2SLS regression, and the diagnostics in the <strong>ivreg</strong> package work with weighted 2SLS fits (see the next section).</p></li>
<li><p>Finally, we can employ a <em>“sandwich” estimator</em> of the coefficient covariance matrix in 2SLS <span class="citation">(or the <em>bootstrap</em>: see, e.g., Davison and Hinkley 1997)</span> to correct standard errors for nonconstant error variance, much as in least-squares regression as proposed by <span class="citation">Huber (1967)</span> and <span class="citation">White (1980; also see Long and Ervin 2000)</span>.</p></li>
</ul>
<p>The <strong>ivreg</strong> package supports the <code>sandwich()</code> function in the <strong>sandwich</strong> package <span class="citation">(Zeileis 2006)</span>. For the Kmenta example, where evidence of nonconstant error variance is slight, the sandwich standard errors are similar to, indeed slightly smaller than, the conventional 2SLS standard errors:</p>
<div class="sourceCode" id="cb49"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="no">deq</span>, <span class="kw">vcov</span><span class="kw">=</span><span class="kw pkg">sandwich</span><span class="kw ns">::</span><span class="no"><a href="https://rdrr.io/pkg/sandwich/man/sandwich.html">sandwich</a></span>)</pre></body></html></div>
<pre><code>## 
## Call:
## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4305 -1.2432 -0.1895  1.5762  2.4920 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 94.63330    5.14745  18.384 1.18e-12 ***
## P           -0.24356    0.07590  -3.209  0.00515 ** 
## D            0.31399    0.04293   7.315 1.21e-06 ***
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   2  16   142.340 6.43e-11 ***
## Wu-Hausman         1  16    21.898 0.000251 ***
## Sargan             1  NA     2.983 0.084137 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.966 on 17 degrees of freedom
## Multiple R-Squared: 0.7548,  Adjusted R-squared: 0.726 
## Wald test: 34.41 on 2 and 17 DF,  p-value: 1.055e-06</code></pre>
<div class="sourceCode" id="cb51"><html><body><pre class="r"><span class="no">SEs</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span>(<span class="kw pkg">sandwich</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/sandwich/man/sandwich.html">sandwich</a></span>(<span class="no">deq</span>))),
                   <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span>(<span class="no">deq</span>)))),
             <span class="fl">4</span>)
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">SEs</span>) <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"sandwich"</span>, <span class="st">"conventional"</span>)
<span class="no">SEs</span></pre></body></html></div>
<pre><code>##             sandwich conventional
## (Intercept)   5.1475       7.9208
## P             0.0759       0.0965
## D             0.0429       0.0469</code></pre>
<p>We’ll modify Kmenta’s data to reflect nonconstant error variance, regenerating the data as Kmenta did originally from the reduced-form equations, expressing the endogenous variables <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> as functions of the exogenous variables <span class="math inline">\(D\)</span>, <span class="math inline">\(F\)</span>, and <span class="math inline">\(A\)</span>, and reduced-form errors <span class="math inline">\(\nu_1\)</span> and <span class="math inline">\(\nu_2\)</span>:</p>
<div class="sourceCode" id="cb53"><html><body><pre class="r"><span class="no">Kmenta2</span> <span class="kw">&lt;-</span> <span class="no">Kmenta</span>[, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"D"</span>, <span class="st">"F"</span>, <span class="st">"A"</span>)]
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span>(<span class="fl">492365</span>) <span class="co"># for reproducibility</span>
<span class="no">Kmenta2</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">within</a></span>(<span class="no">Kmenta2</span>, {
    <span class="no">EQ</span> <span class="kw">&lt;-</span> <span class="fl">75.25</span> + <span class="fl">0.1125</span>*<span class="no">D</span> + <span class="fl">0.1250</span>*<span class="no">F</span> + <span class="fl">0.225</span>*<span class="no">A</span>
    <span class="no">EP</span> <span class="kw">&lt;-</span> <span class="fl">85.00</span> + <span class="fl">0.7500</span>*<span class="no">D</span> - <span class="fl">0.5000</span>*<span class="no">F</span> - <span class="fl">0.900</span>*<span class="no">A</span>
    <span class="no">d1</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="fl">20</span>)
    <span class="no">d2</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span>(<span class="fl">20</span>)
    <span class="no">v1</span> <span class="kw">&lt;-</span> <span class="fl">2</span>*<span class="no">d1</span>
    <span class="no">v2</span> <span class="kw">&lt;-</span> -<span class="fl">0.5</span>*<span class="no">v1</span> + <span class="no">d2</span>
    <span class="no">w</span> <span class="kw">&lt;-</span> <span class="fl">3</span>*(<span class="no">EQ</span> - <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span>(<span class="no">EQ</span>) + <span class="fl">0.1</span>)/(<span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span>(<span class="no">EQ</span>) - <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span>(<span class="no">EQ</span>))
    <span class="no">v1</span> <span class="kw">&lt;-</span> <span class="no">v1</span>*<span class="no">w</span> <span class="co"># inducing nonconstant variance</span>
    <span class="no">Q</span> <span class="kw">&lt;-</span> <span class="no">EQ</span> + <span class="no">v1</span>
    <span class="no">P</span> <span class="kw">&lt;-</span> <span class="no">EP</span> + <span class="no">v2</span>
})</pre></body></html></div>
<p>Plotting the sampled reduced-form errors <code>v1</code> against the expectation of <code>Q</code> shows a clear heterscedastic pattern:</p>
<div class="sourceCode" id="cb54"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span>(<span class="no">Kmenta2</span>, <span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="no">EQ</span>, <span class="no">v1</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-28-1.png" width="384"></p>
<p>Then refitting the demand equation to the new data set, we get</p>
<div class="sourceCode" id="cb55"><html><body><pre class="r"><span class="no">deq2</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span>(<span class="no">deq</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">Kmenta2</span>)
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="no">deq2</span>)</pre></body></html></div>
<pre><code>## 
## Call:
## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7805 -1.4920  0.1067  1.9745  5.0629 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 111.42774   11.36644   9.803 2.07e-08 ***
## P            -0.39072    0.13979  -2.795  0.01243 *  
## D             0.28415    0.07965   3.567  0.00237 ** 
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   2  16   122.011 2.06e-10 ***
## Wu-Hausman         1  16    22.883 0.000203 ***
## Sargan             1  NA     0.221 0.638221    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.283 on 17 degrees of freedom
## Multiple R-Squared: 0.521,   Adjusted R-squared: 0.4646 
## Wald test: 6.695 on 2 and 17 DF,  p-value: 0.007172</code></pre>
<p>and the nonconstant error variance is clearly reflected in diagnostics; for example,</p>
<div class="sourceCode" id="cb57"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/spreadLevelPlot.html">spreadLevelPlot</a></span>(<span class="no">deq2</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-30-1.png" width="384"></p>
<pre><code>## 
## Suggested power transformation:  -22.57328</code></pre>
<div class="sourceCode" id="cb59"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html">ncvTest</a></span>(<span class="no">deq2</span>)</pre></body></html></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 6.690435, Df = 1, p = 0.0096932</code></pre>
<p>The extreme value of the suggested power transformation of <span class="math inline">\(Q\)</span> from the spread-level plot, <span class="math inline">\(\lambda =-23\)</span>, reflects (as we noted previously) the fact that <span class="math inline">\(\max(Q)/\min(Q)\)</span> isn’t much larger than 1.</p>
<p>In our example, the sandwich standard errors are not very different from the conventional standard errors:</p>
<div class="sourceCode" id="cb61"><html><body><pre class="r"><span class="no">SEs2</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span>(<span class="kw pkg">sandwich</span><span class="kw ns">::</span><span class="fu"><a href="https://rdrr.io/pkg/sandwich/man/sandwich.html">sandwich</a></span>(<span class="no">deq2</span>))),
                   <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>(<span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span>(<span class="no">deq2</span>)))),
             <span class="fl">4</span>)
<span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span>(<span class="no">SEs2</span>) <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span>(<span class="st">"sandwich"</span>, <span class="st">"conventional"</span>)
<span class="no">SEs2</span></pre></body></html></div>
<pre><code>##             sandwich conventional
## (Intercept)  13.7782      11.3664
## P             0.1702       0.1398
## D             0.0848       0.0797</code></pre>
<p>As mentioned, bootstrapping provides an alternative to sandwich standard errors as a correction for nonconstant error variance, and the <strong>ivreg</strong> package supplies an <code>"ivreg"</code> method for the <code><a href="https://rdrr.io/pkg/car/man/Boot.html">Boot()</a></code> function in the <strong>car</strong> package, implementing the case-resampling bootstrap, and returning an object of class <code>"boot"</code> suitable for use with functions in the <strong>boot</strong> package <span class="citation">(Davison and Hinkley 1997; Canty and Ripley 2020)</span>. By default, <span class="math inline">\(R = 999\)</span> bootstrap replications are generated. For example:</p>
<div class="sourceCode" id="cb63"><html><body><pre class="r"><span class="no">set.seed</span> <span class="kw">&lt;-</span> <span class="fl">869255</span> <span class="co"># for reproducibility</span>
<span class="no">b.deq2</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/car/man/Boot.html">Boot</a></span>(<span class="no">deq2</span>)
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="no">deq2</span>, <span class="kw">vcov.</span><span class="kw">=</span><span class="fu"><a href="https://rdrr.io/r/stats/vcov.html">vcov</a></span>(<span class="no">b.deq2</span>))</pre></body></html></div>
<pre><code>## 
## Call:
## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.7805 -1.4920  0.1067  1.9745  5.0629 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 111.42774   16.70085   6.672 3.93e-06 ***
## P            -0.39072    0.19678  -1.986  0.06345 .  
## D             0.28415    0.09762   2.911  0.00974 ** 
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   2  16   122.011 2.06e-10 ***
## Wu-Hausman         1  16    22.883 0.000203 ***
## Sargan             1  NA     0.221 0.638221    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.283 on 17 degrees of freedom
## Multiple R-Squared: 0.521,   Adjusted R-squared: 0.4646 
## Wald test:  4.32 on 2 and 17 DF,  p-value: 0.03042</code></pre>
<p>The bootstrap standard errors are larger than the conventional or sandwich standard errors for this example.</p>
<p>Boostrap confidence intervals can also be computed from the object returned by <code><a href="https://rdrr.io/pkg/car/man/Boot.html">Boot()</a></code>, by default reporting <span class="math inline">\(BC_a\)</span> (<em>bias-corrected, accelerated</em>) intervals (see the documentation for <code><a href="https://rdrr.io/pkg/car/man/hist.boot.html">confint.boot()</a></code> in the <strong>car</strong> package):</p>
<div class="sourceCode" id="cb65"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/stats/confint.html">confint</a></span>(<span class="no">b.deq2</span>)</pre></body></html></div>
<pre><code>## Bootstrap bca confidence intervals
## 
##                   2.5 %      97.5 %
## (Intercept) 64.38447188 137.8541642
## P           -0.61242364   0.2463746
## D            0.05739467   0.4253580</code></pre>
</div>
<div id="weighted-2sls-regession" class="section level2">
<h2 class="hasAnchor">
<a href="#weighted-2sls-regession" class="anchor"></a>Weighted 2SLS Regession</h2>
<p>Suppose that we modify the regression model <span class="math inline">\(y = X \beta + \varepsilon\)</span> so that now <span class="math inline">\(N_n(0, \sigma^2 W^{-1})\)</span> where <span class="math inline">\(W = \mathrm{diag}\{w_i\}\)</span> is an <span class="math inline">\(n \times n\)</span> diagonal matrix of known inverse-variance weights; that is <span class="math inline">\(V(\varepsilon_i) = \sigma^2/w_i\)</span>. As before, some of the columns of <span class="math inline">\(X\)</span> may be correlated with the errors <span class="math inline">\(\varepsilon\)</span>, but we have sufficient instrumental variables <span class="math inline">\(Z\)</span> that are independent of the errors.</p>
<p>Then the <em>weighted 2SLS</em> estimator is <span class="math display">\[
b_{\mathrm{W2SLS}} = [X^\top W Z(Z^\top W Z)^{-1} Z^\top W X]^{-1} X^\top W Z (Z^\top W Z)^{-1} Z^\top W y
\]</span> Alternatively, we can treat the two stages of 2SLS as <em>weighted least squares (WLS)</em> problems, in each stage minimizing the weighted sum of squared residuals. The <code><a href="../reference/ivreg.html">ivreg()</a></code> function computes the weighted 2SLS estimator in this manner.</p>
<p>Phillips’s updating formulas for 2SLS regression could also be modified for the weighted case, but a simpler approach (which is evident in the formula for <span class="math inline">\(b_{\mathrm{W2SLS}}\)</span> above) is to convert the weighted 2SLS problem into an unweighted problem, by transforming the data to constant variance using <span class="math inline">\(W^{1/2} = \mathrm{diag}\{\sqrt{w_i}\}\)</span>, the Cholesky square root of <span class="math inline">\(W\)</span>. The square root of <span class="math inline">\(W\)</span> is particularly simple because <span class="math inline">\(W\)</span> is diagonal. Then in Phillips’s updating formulas, we replace <span class="math inline">\(y\)</span> with <span class="math inline">\(y^* = W^{1/2}y\)</span>, <span class="math inline">\(X\)</span> with <span class="math inline">\(X^* = W^{1/2}X\)</span>, and <span class="math inline">\(Z\)</span> with <span class="math inline">\(Z^* = W^{1/2}Z\)</span>.</p>
<p>For the modified <code>Kmenta2</code> data, we know that the variances of the errors in the demand equation are inversely proportional to the variable <code>w</code>. This is of course artificial knowledge, reflecting the manner in which the data were constructed. The weighted 2SLS estimator is therefore computed as</p>
<div class="sourceCode" id="cb67"><html><body><pre class="r"><span class="no">deqw</span> <span class="kw">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span>(<span class="no">deq</span>, <span class="kw">data</span><span class="kw">=</span><span class="no">Kmenta2</span>, <span class="kw">weights</span><span class="kw">=</span><span class="fl">1</span>/<span class="no">w</span>)
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span>(<span class="no">deqw</span>)</pre></body></html></div>
<pre><code>## 
## Call:
## ivreg(formula = Q ~ P + D | D + F + A, data = Kmenta2, weights = 1/w)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -5.43959 -1.66625 -0.08906  1.81440  3.41694 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 107.88374   10.23415  10.542 7.11e-09 ***
## P            -0.33586    0.12240  -2.744   0.0138 *  
## D             0.26347    0.04405   5.981 1.49e-05 ***
## 
## Diagnostic tests:
##                  df1 df2 statistic  p-value    
## Weak instruments   2  16   101.172 8.31e-10 ***
## Wu-Hausman         1  16    20.105 0.000376 ***
## Sargan             1  NA     0.087 0.767864    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.308 on 17 degrees of freedom
## Multiple R-Squared: 0.7166,  Adjusted R-squared: 0.6833 
## Wald test: 18.79 on 2 and 17 DF,  p-value: 4.95e-05</code></pre>
<p>Plotting studentized residuals against fitted values and testing for nonconstant error variance don’t indicate a heteroscedasticity problem, but there is a relatively large studentized residual of about <span class="math inline">\(-3\)</span> that stands out somewhat from the other values:</p>
<div class="sourceCode" id="cb69"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/ncvTest.html">ncvTest</a></span>(<span class="no">deqw</span>)</pre></body></html></div>
<pre><code>## Non-constant Variance Score Test 
## Variance formula: ~ fitted.values 
## Chisquare = 4.21029, Df = 1, p = 0.040179</code></pre>
<div class="sourceCode" id="cb71"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/plot.html">plot</a></span>(<span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html">fitted</a></span>(<span class="no">deqw</span>), <span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html">rstudent</a></span>(<span class="no">deqw</span>))</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-36-1.png" width="384"></p>
<p>A Bonferroni outlier test suggests that the studentized residual isn’t unusually large, and once more we’re in the unusual situation of knowing that the model is correct.</p>
<div class="sourceCode" id="cb72"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/outlierTest.html">outlierTest</a></span>(<span class="no">deqw</span>)</pre></body></html></div>
<pre><code>## No Studentized residuals with Bonferroni p &lt; 0.05
## Largest |rstudent|:
##       rstudent unadjusted p-value Bonferroni p
## 1937 -3.135343          0.0063887      0.12777</code></pre>
</div>
<div id="collinearity-diagnostics" class="section level2">
<h2 class="hasAnchor">
<a href="#collinearity-diagnostics" class="anchor"></a>Collinearity Diagnostics</h2>
<p>In addition to unusual-data diagnostics, <span class="citation">Belsley, Kuh, and Welsch (1980)</span> briefly extend their approach to collinearity diagnostics to 2SLS regression. We believe that this approach, which assimilates collinearity to numerical instability, is flawed, in that it takes into account “collinearity with the intercept.” That is, regressors with values far from 0 have large sums of products with the constant regressor, producing a large standard error of the intercept, and simply reflecting the fact that the intercept extrapolates the fitted regression surface far beyond the range of the data.</p>
<p><span class="citation">Fox and Monette (1992)</span> describe an alternative approach to collinearity diagnostics in linear models fit by least squares based on <em>generalized variance-inflation factors</em>. The implementation of generalized variance inflation fators in the <code><a href="https://rdrr.io/pkg/car/man/vif.html">vif()</a></code> function in the <strong>car</strong> package, which employs the estimated covariance matrix of the coefficients, applies in general to models with linear predictors, including linear models estimated by 2SLS.</p>
<p>For example, for the demand equation in Kmenta’s model:</p>
<div class="sourceCode" id="cb74"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span>(<span class="fu"><a href="https://rdrr.io/pkg/car/man/vif.html">vif</a></span>(<span class="no">deq</span>))</pre></body></html></div>
<pre><code>##        P        D 
## 1.231124 1.231124</code></pre>
<p>Taking the square-roots of the VIFs puts them on the coefficient standard-error scale. That is, the standard errors of the coefficients of <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> are 23% larger than they would be if the estimated coefficients were uncorrelated (which is equivalent to the columns of <span class="math inline">\(\widehat{X}\)</span> for <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> in the second-stage regression being uncorrelated). When, as here, each term in the model has just one coefficient, generalized and ordinary variance-inflation factors coincide. The equality of the VIFs for <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> is peculiar to the case of two regressors (beyond the regression constant).</p>
<p><em>Marginal/conditional</em> plots, produced by the <code><a href="https://rdrr.io/pkg/car/man/mcPlots.html">mcPlots()</a></code> function in the <strong>car</strong> package, superimpose the added-variable plots on corresponding marginal scatterplots for the regressors. These graphs allow us to visualize the reduction in precision of estimation of each coefficient due to collinearity, which reduces the conditional variation of a regressor relative to its marginal variation. for example, for the demand equation:</p>
<div class="sourceCode" id="cb76"><html><body><pre class="r"><span class="fu"><a href="https://rdrr.io/pkg/car/man/mcPlots.html">mcPlots</a></span>(<span class="no">deq</span>)</pre></body></html></div>
<p><img src="Diagnostics-for-2SLS-Regression_files/figure-html/unnamed-chunk-39-1.png" width="768"></p>
<p>The blue points in each panel represent the marginal scatterplot and the magenta points represent the (partial) added-variable plot, with the arrows showing the relationship between the two sets of points.</p>
</div>
<div id="concluding-remarks" class="section level2">
<h2 class="hasAnchor">
<a href="#concluding-remarks" class="anchor"></a>Concluding Remarks</h2>
<p>Careful regression analysis requires methods for looking effectively at the data. Many potential problems can be addressed by examining the data <em>prior</em> to fitting a regression model, decreasing (if not eliminating) the necessity for <em>post-fit</em> diagnostics. No doubt careful data analysts employing 2SLS have always done this. Nevertheless, having methods that allow one to subject a regression model fit by 2SLS to criticism will in at least some cases suggest improvements to the model or perhaps corrections to the data.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-Basmann1957">
<p>Basmann, R. L. 1957. “A Generalized Classical Method of Linear Estimation of Coefficients in a Structural Equation.” <em>Econometrica</em> 25: 77–83. <a href="https://doi.org/10.2307/1907743">https://doi.org/10.2307/1907743</a>.</p>
</div>
<div id="ref-BelsleyKuhWelsch1980">
<p>Belsley, D. A., E. Kuh, and R. E. Welsch. 1980. <em>Regression Diagnostics: Identifying Influential Data and Sources of Collinearity</em>. New York: John Wiley &amp; Sons. <a href="https://doi.org/10.1002/0471725153">https://doi.org/10.1002/0471725153</a>.</p>
</div>
<div id="ref-BreuschPagan1979">
<p>Breusch, T. S., and A. R. Pagan. 1979. “A Simple Test for Heteroscedasticity and Random Coefficient Variation.” <em>Econometrica</em> 47: 1287–94. <a href="https://doi.org/10.2307/1911963">https://doi.org/10.2307/1911963</a>.</p>
</div>
<div id="ref-boot">
<p>Canty, Angelo, and Brian D. Ripley. 2020. <em>Boot: Bootstrap R (S-PLUS) Functions</em>. <a href="https://CRAN.R-project.org/package=boot">https://CRAN.R-project.org/package=boot</a>.</p>
</div>
<div id="ref-ClevelandGrosseShyu1992">
<p>Cleveland, W. S., E. Grosse, and W. M. Shyu. 1992. “Local Regression Models.” In <em>Statistical Models in S</em>, edited by J.M. Chambers and T.J. Hastie, 309–76. Pacific Grove CA: Wadsworth &amp; Brooks/Cole.</p>
</div>
<div id="ref-Cook1993">
<p>Cook, R. D. 1993. “Exploring Partial Residual Plots.” <em>Technometrics</em> 35: 351–62. <a href="https://doi.org/10.1080/00401706.1993.10485350">https://doi.org/10.1080/00401706.1993.10485350</a>.</p>
</div>
<div id="ref-CookCroosDabrera1998">
<p>Cook, R. D., and R. Croos-Dabrera. 1998. “Partial Residual Plots in Generalized Linear Models.” <em>Journal of the American Statistical Association</em> 93: 730–39. <a href="https://doi.org/10.1080/01621459.1998.10473725">https://doi.org/10.1080/01621459.1998.10473725</a>.</p>
</div>
<div id="ref-CookWeisberg1983">
<p>Cook, R. D., and S. Weisberg. 1983. “Diagnostics for Heteroscedasticity in Regression.” <em>Biometrika</em> 70: 1–10. <a href="https://doi.org/10.1093/biomet/70.1.1">https://doi.org/10.1093/biomet/70.1.1</a>.</p>
</div>
<div id="ref-DavisonHinkley1997">
<p>Davison, A. C., and D. V. Hinkley. 1997. <em>Bootstrap Methods and Their Applications</em>. Cambridge: Cambridge University Press.</p>
</div>
<div id="ref-Fox2016">
<p>Fox, J. 2016. <em>Applied Regression Analysis and Generalized Linear Models</em>. 3rd ed. Thousand Oaks: Sage.</p>
</div>
<div id="ref-FoxMonette1992">
<p>Fox, J., and G. Monette. 1992. “Generalized Collinearity Diagnostics.” <em>Journal of the American Statistical Association</em>, 178–83. <a href="https://doi.org/10.1080/01621459.1992.10475190">https://doi.org/10.1080/01621459.1992.10475190</a>.</p>
</div>
<div id="ref-sem">
<p>Fox, J., Z. Nie, and J. Byrnes. 2020. <em>sem: Structural Equation Models</em>. <a href="https://CRAN.R-project.org/package=sem">https://CRAN.R-project.org/package=sem</a>.</p>
</div>
<div id="ref-FoxWeisberg2018">
<p>Fox, J., and S. Weisberg. 2018. “Visualizing Fit and Lack of Fit in Complex Regression Models with Predictor Effect Plots and Partial Residuals.” <em>Journal of Statistical Software</em> 87 (9): 1–27. <a href="https://doi.org/10.18637/jss.v087.i09">https://doi.org/10.18637/jss.v087.i09</a>.</p>
</div>
<div id="ref-FoxWeisberg2019">
<p>———. 2019. <em>An R Companion to Applied Regression</em>. 3rd ed. Thousand Oaks: Sage.</p>
</div>
<div id="ref-Huber1967">
<p>Huber, P. J. 1967. “The Behavior of Maximum Likelihood Estimation Under Nonstandard Conditions.” In <em>Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</em>, edited by L. M. LeCam and J. Neyman. Berkeley: University of California Press.</p>
</div>
<div id="ref-KleiberZeileis2008">
<p>Kleiber, C., and A. Zeileis. 2008. <em>Applied Econometrics with R</em>. New York: Springer-Verlag.</p>
</div>
<div id="ref-Kmenta1986">
<p>Kmenta, J. 1986. <em>Elements of Econometrics</em>. 2nd ed. New York: Macmillan.</p>
</div>
<div id="ref-LongErvin2000">
<p>Long, J. S., and L. H. Ervin. 2000. “Using Heteroscedasticity Consistent Standard Errors in the Linear Regression Model.” <em>The American Statistician</em> 54: 217–24. <a href="https://doi.org/10.1080/00031305.2000.10474549">https://doi.org/10.1080/00031305.2000.10474549</a>.</p>
</div>
<div id="ref-Phillips1977">
<p>Phillips, G. D. A. 1977. “Recursions for the Two-Stage Least-Squares Estimators.” <em>Journal of Econometrics</em> 6: 65–77. <a href="https://doi.org/10.1016/0304-4076(77)90055-0">https://doi.org/10.1016/0304-4076(77)90055-0</a>.</p>
</div>
<div id="ref-Pregibon1981">
<p>Pregibon, D. 1981. “Logistic Regression Diagnostics.” <em>The Annals of Statistics</em> 9: 705–24. <a href="https://doi.org/10.1214/aos/1176345513">https://doi.org/10.1214/aos/1176345513</a>.</p>
</div>
<div id="ref-R">
<p>R Core Team. 2020. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-Theil1971">
<p>Theil, H. 1971. <em>Principles of Econometrics</em>. New York: John Wiley &amp; Sons.</p>
</div>
<div id="ref-Tukey1977">
<p>Tukey, J. W. 1977. <em>Exploratory Data Analysis</em>. Reading: Addison-Wesley.</p>
</div>
<div id="ref-White1980">
<p>White, H. 1980. “A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity.” <em>Econometrica</em> 48: 817–38. <a href="https://doi.org/10.2307/1912934">https://doi.org/10.2307/1912934</a>.</p>
</div>
<div id="ref-Zeileis2006">
<p>Zeileis, A. 2006. “Object-Oriented Computation of Sandwich Estimators.” <em>Journal of Statistical Software</em> 16 (9): 1–16. <a href="https://doi.org/10.18637/jss.v016.i09">https://doi.org/10.18637/jss.v016.i09</a>.</p>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by John Fox, Christian Kleiber, Achim Zeileis.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
